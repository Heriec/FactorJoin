{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53641ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from Join_scheme.data_prepare import process_stats_data\n",
    "from BayesCard.Models.Bayescard_BN import Bayescard_BN\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from BayesCard.Evaluation.cardinality_estimation import parse_query_single_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b523b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Join_scheme.data_prepare import process_stats_data\n",
    "data_path = \"/home/ubuntu/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "model_folder = \"/home/ubuntu/data_CE/saved_models\"\n",
    "data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size = process_stats_data(data_path,\n",
    "                                                model_folder, 200, \"sub_optimal\", get_bin_means=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from Join_scheme.join_graph import process_condition, get_join_hyper_graph\n",
    "from Join_scheme.data_prepare import identify_key_values\n",
    "from BayesCard.Evaluation.cardinality_estimation import timestamp_transorform, construct_table_query\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, variables, pdfs, equivalent_variables=[]):\n",
    "        self.variables = variables\n",
    "        self.equivalent_variables = equivalent_variables\n",
    "        self.pdfs = pdfs\n",
    "        self.cardinalities = dict()\n",
    "        for i, var in enumerate(self.variables):\n",
    "            self.cardinalities[var] = pdfs.shape[i]\n",
    "            if len(equivalent_variables) != 0:\n",
    "                self.cardinalities[equivalent_variables[i]] = pdfs.shape[i]\n",
    "\n",
    "\n",
    "class Bound_ensemble:\n",
    "    \"\"\"\n",
    "    This the class where we store all the trained models and perform inference on the bound.\n",
    "    \"\"\"\n",
    "    def __init__(self, bns, all_bin_means, schema):\n",
    "        self.bns = bns\n",
    "        self.table_buckets = all_bin_means\n",
    "        self.schema = schema\n",
    "        self.all_keys, self.equivalent_keys = identify_key_values(schema)\n",
    "\n",
    "    def parse_query_simple(self, query):\n",
    "        \"\"\"\n",
    "        If your selection query contains no aggregation and nested sub-queries, you can use this function to parse a\n",
    "        join query. Otherwise, use parse_query function.\n",
    "        \"\"\"\n",
    "        query = query.replace(\" where \", \" WHERE \")\n",
    "        query = query.replace(\" from \", \" FROM \")\n",
    "        query = query.replace(\" and \", \" AND \")\n",
    "        query = query.split(\";\")[0]\n",
    "        query = query.strip()\n",
    "        tables_all = {}\n",
    "        join_cond = []\n",
    "        table_query = {}\n",
    "        join_keys = {}\n",
    "        tables_str = query.split(\" WHERE \")[0].split(\" FROM \")[-1]\n",
    "        for table_str in tables_str.split(\",\"):\n",
    "            table_str = table_str.strip()\n",
    "            if \" as \" in table_str:\n",
    "                tables_all[table_str.split(\" as \")[-1]] = table_str.split(\" as \")[0]\n",
    "            else:\n",
    "                tables_all[table_str.split(\" \")[-1]] = table_str.split(\" \")[0]\n",
    "\n",
    "        # processing conditions\n",
    "        conditions = query.split(\" WHERE \")[-1].split(\" AND \")\n",
    "        for cond in conditions:\n",
    "            table, cond, join, join_key = process_condition(cond, tables_all)\n",
    "            if not join:\n",
    "                attr = cond[0]\n",
    "                op = cond[1]\n",
    "                value = cond[2]\n",
    "                if \"Date\" in attr:\n",
    "                    assert \"::timestamp\" in value\n",
    "                    value = timestamp_transorform(value.strip().split(\"::timestamp\")[0])\n",
    "                if table not in table_query:\n",
    "                    table_query[table] = dict()\n",
    "                construct_table_query(self.bns[table], table_query[table], attr, op, value)\n",
    "            else:\n",
    "                join_cond.append(cond)\n",
    "                for tab in join_key:\n",
    "                    if tab in join_keys:\n",
    "                        join_keys[tab].add(join_key[tab])\n",
    "                    else:\n",
    "                        join_keys[tab] = set([join_key[tab]])\n",
    "\n",
    "        return tables_all, table_query, join_cond, join_keys\n",
    "\n",
    "    def get_all_id_conidtional_distribution(self, table_queries, join_keys, equivalent_group):\n",
    "        res = dict()\n",
    "        for table in join_keys:\n",
    "            key_attrs = list(join_keys[table])\n",
    "            if table in table_queries:\n",
    "                table_query = table_queries[table]\n",
    "            else:\n",
    "                table_query = {}\n",
    "            id_attrs, probs = self.bns[table].query_id_prob(table_query, key_attrs)\n",
    "            new_id_attrs = []\n",
    "            for K in id_attrs:\n",
    "                for PK in equivalent_group:\n",
    "                    if K in equivalent_group[PK]:\n",
    "                        new_id_attrs.append(PK)\n",
    "            assert len(new_id_attrs) == len(id_attrs)\n",
    "            res[table] = Factor(id_attrs, probs, new_id_attrs)\n",
    "        return res\n",
    "\n",
    "    def eliminate_one_key_group(self, tables, key_group, factors, relevant_keys):\n",
    "        \"\"\"This version only supports 2D distributions\"\"\"\n",
    "        rest_group = None\n",
    "        rest_group_cardinalty = 0\n",
    "        eliminated_tables = []\n",
    "        rest_group_tables = []\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            if len(temp) == 0:\n",
    "                eliminated_tables.append(table)\n",
    "            for key in temp:\n",
    "                if rest_group:\n",
    "                    assert factors[table].cardinalities[key] == rest_group_cardinalty\n",
    "                    rest_group_tables.append(table)\n",
    "                else:\n",
    "                    rest_group = key\n",
    "                    rest_group_cardinalty = factors[table].cardinalities[key]\n",
    "                    rest_group_tables = [table]\n",
    "\n",
    "        all_probs_eliminated = []\n",
    "        all_modes_eliminated = []\n",
    "        for table in eliminated_tables:\n",
    "            bin_modes = self.table_buckets[table].oned_bin_modes[relevant_keys[key_group][table]]\n",
    "            all_probs_eliminated.append(factors[table].pdfs)\n",
    "            all_modes_eliminated.append(np.minimum(bin_modes, factors[table].pdfs))\n",
    "        if rest_group:\n",
    "            new_factor_pdf = np.zeros(rest_group_cardinalty)\n",
    "        else:\n",
    "            return self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for i in range(rest_group_cardinalty):\n",
    "            rest_group_probs_eliminated = []\n",
    "            rest_group_modes_eliminated = []\n",
    "            for table in rest_group_tables:\n",
    "\n",
    "                idx_f = factors[table].equivalent_variables.index(key_group)\n",
    "                idx_b = self.table_buckets[table].id_attributes.index(relevant_keys[key_group][table])\n",
    "                bin_modes = self.table_buckets[table].twod_bin_modes[relevant_keys[key_group][table]]\n",
    "                if idx_f == 0 and idx_b == 0:\n",
    "                    rest_group_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    rest_group_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 0 and idx_b == 1:\n",
    "                    rest_group_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    rest_group_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 1 and idx_b == 0:\n",
    "                    rest_group_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    rest_group_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[i, :]))\n",
    "                else:\n",
    "                    rest_group_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    rest_group_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[i, :]))\n",
    "            new_factor_pdf[i] = self.compute_bound_oned(all_probs_eliminated + rest_group_probs_eliminated,\n",
    "                                                        all_modes_eliminated + rest_group_modes_eliminated)\n",
    "\n",
    "        for table in rest_group_tables:\n",
    "            factors[table] = Factor([rest_group], new_factor_pdf, [rest_group])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def compute_bound_oned(self, all_probs, all_modes):\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        all_modes = np.stack(all_modes, axis=0)\n",
    "        multiplier = np.prod(all_modes, axis=0)\n",
    "        non_zero_idx = np.where(multiplier != 0)[0]\n",
    "        min_number = np.amin(all_probs[:, non_zero_idx]/all_modes[:, non_zero_idx], axis=0)\n",
    "        multiplier[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "        return np.sum(multiplier)\n",
    "\n",
    "    def get_optimal_elimination_order(self, equivalent_group, join_keys, factors):\n",
    "        cardinalities = dict()\n",
    "        lengths = dict()\n",
    "        tables_involved = dict()\n",
    "        relevant_keys = dict()\n",
    "        for group in equivalent_group:\n",
    "            relevant_keys[group] = dict()\n",
    "            lengths[group] = len(equivalent_group[group])\n",
    "            cardinalities[group] = []\n",
    "            tables_involved[group] = set([])\n",
    "            for keys in equivalent_group[group]:\n",
    "                for table in join_keys:\n",
    "                    if keys in join_keys[table]:\n",
    "                        cardinalities[group].append(len(join_keys[table]))\n",
    "                        tables_involved[group].add(table)\n",
    "                        variables = factors[table].variables\n",
    "                        variables[variables.index(keys)] = group\n",
    "                        factors[table].variables = variables\n",
    "                        relevant_keys[group][table] = keys\n",
    "                        break\n",
    "            cardinalities[group] = np.asarray(cardinalities[group])\n",
    "\n",
    "        optimal_order = list(equivalent_group.keys())\n",
    "        for i in range(len(optimal_order)):\n",
    "            min_idx = i\n",
    "            for j in range(i+1, len(optimal_order)):\n",
    "                min_group = optimal_order[min_idx]\n",
    "                curr_group = optimal_order[j]\n",
    "                if np.max(cardinalities[curr_group]) < np.max(cardinalities[min_group]):\n",
    "                    min_idx = j\n",
    "                else:\n",
    "                    min_max_tables = np.max(cardinalities[min_group])\n",
    "                    min_num_max_tables = len(np.where(cardinalities[min_group] == min_max_tables)[0])\n",
    "                    curr_max_tables = np.max(cardinalities[curr_group])\n",
    "                    curr_num_max_tables = len(np.where(cardinalities[curr_group] == curr_max_tables)[0])\n",
    "                    if curr_num_max_tables < min_num_max_tables:\n",
    "                        min_idx = j\n",
    "                    elif lengths[curr_group] < lengths[min_group]:\n",
    "                        min_idx = j\n",
    "            optimal_order[i], optimal_order[min_idx] = optimal_order[min_idx], optimal_order[i]\n",
    "        return optimal_order, tables_involved, relevant_keys\n",
    "\n",
    "    def get_cardinality(self, query_str):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group = get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(table_queries, join_keys, equivalent_group)\n",
    "        optimal_order, tables_involved, relevant_keys = self.get_optimal_elimination_order(equivalent_group, join_keys,\n",
    "                                                                            conditional_factors)\n",
    "\n",
    "        for key_group in optimal_order:\n",
    "            tables = tables_involved[key_group]\n",
    "            res = self.eliminate_one_key_group(tables, key_group, conditional_factors, relevant_keys)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/data_CE/CE_scheme_models/model_stats_sub_optimal_200.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    old_BE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BE = Bound_ensemble(old_BE.bns, table_buckets, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = \"/home/ubuntu/End-to-End-CardEst-Benchmark/workloads/stats_CEB/sub_plan_queries/stats_CEB_sub_queries.sql\"\n",
    "with open(query_file, \"r\") as f:\n",
    "    queries = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5394b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = []\n",
    "latency = []\n",
    "pred = []\n",
    "for i, query_str in enumerate(queries):\n",
    "    #if i == 10: break\n",
    "    query = query_str.split(\"||\")[0][:-1]\n",
    "    print(\"========================\")\n",
    "    true_card = int(query_str.split(\"||\")[-1])\n",
    "    t = time.time()\n",
    "    res = BE.get_cardinality(query)\n",
    "    pred.append(res)\n",
    "    latency.append(time.time() - t)\n",
    "    qerror.append(res/true_card)\n",
    "    print(f\"estimating query {i}: predicted {res}, true_card {true_card}, qerror {res/true_card}, latency {time.time() - t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81df12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(f\"q-error {i}% percentile is {np.percentile(qerror, i)}\")\n",
    "print(f\"total inference time: {np.sum(latency)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7467a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_no_bound.txt\", \"w\") as f:\n",
    "    for p in pred:\n",
    "        f.write(str(p)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5ff95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
