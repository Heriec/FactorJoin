{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/CE_scheme/\")\n",
    "from Schemas.stats.schema import gen_stats_light_schema\n",
    "from Join_scheme.data_prepare import read_table_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d51dec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy import stats\n",
    "import jenkspy\n",
    "\n",
    "\n",
    "class Bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization of a key attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bins=[], bin_modes=[], bin_vars=[], bin_means=[], rest_bins_remaining=None):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        self.bin_modes = bin_modes\n",
    "        self.bin_vars = bin_vars\n",
    "        self.bin_means = bin_means\n",
    "        self.rest_bins_remaining = rest_bins_remaining\n",
    "        if len(bins) != 0:\n",
    "            assert len(bins) == len(bin_modes)\n",
    "\n",
    "\n",
    "class Table_bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization for all key attributes in a table.\n",
    "    Supporting more than three dimensional bin modes requires simplifying the causal structure, which is left as a\n",
    "    future work.\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name, id_attributes, bin_sizes):\n",
    "        self.table_name = table_name\n",
    "        self.id_attributes = id_attributes\n",
    "        self.bin_sizes = bin_sizes\n",
    "        self.oned_bin_modes = dict()\n",
    "        self.twod_bin_modes = dict()\n",
    "\n",
    "\n",
    "class Bucket_group:\n",
    "    \"\"\"\n",
    "    The class of bucketization for a group of equivalent join keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buckets, start_key, sample_rate, bins=None, primary_keys=[]):\n",
    "        self.buckets = buckets\n",
    "        self.start_key = start_key\n",
    "        self.sample_rate = sample_rate\n",
    "        self.bins = bins\n",
    "        self.primary_keys = primary_keys\n",
    "\n",
    "    def bucketize(self, data):\n",
    "        \"\"\"\n",
    "        Discretize data based on the bucket\n",
    "        \"\"\"\n",
    "        res = dict()\n",
    "        seen_remain_key = np.array([])\n",
    "        cumulative_bin = copy.deepcopy(self.buckets[self.start_key].bins)\n",
    "        start_means = np.asarray(self.buckets[self.start_key].bin_means)\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            if key != self.start_key:\n",
    "                unique_remain = np.setdiff1d(self.buckets[key].rest_bins_remaining, seen_remain_key)\n",
    "                assert sum([np.sum(np.isin(unique_remain, b) == 1) for b in cumulative_bin]) == 0\n",
    "\n",
    "                if len(unique_remain) != 0:\n",
    "                    remaining_data = data[key][np.isin(data[key], unique_remain)]\n",
    "                    unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "                    unique_counts = np.unique(count_remain)\n",
    "                    for u in unique_counts:\n",
    "                        temp_idx = np.searchsorted(start_means, u)\n",
    "                        if temp_idx == len(cumulative_bin):\n",
    "                            idx = -1\n",
    "                            if u > self.buckets[key].bin_modes[-1]:\n",
    "                                self.buckets[key].bin_modes[-1] = u\n",
    "                        elif temp_idx == 0:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            if (u - start_means[temp_idx - 1]) >= (start_means[temp_idx] - u):\n",
    "                                idx = temp_idx - 1\n",
    "                            else:\n",
    "                                idx = temp_idx\n",
    "                        temp_unique = unique_remain[count_remain == u]\n",
    "                        cumulative_bin[idx] = np.concatenate((cumulative_bin[idx], temp_unique))\n",
    "                        seen_remain_key = np.concatenate((seen_remain_key, temp_unique))\n",
    "                        if u > self.buckets[key].bin_modes[idx]:\n",
    "                            self.buckets[key].bin_modes[idx] = u\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            count = 0\n",
    "            for i, b in enumerate(cumulative_bin):\n",
    "                count += len(data[key][np.isin(data[key], b)])\n",
    "                res[key][np.isin(data[key], b)] = i\n",
    "        self.bins = cumulative_bin\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                res[key] = self.bucketize_PK(data[key])\n",
    "                self.buckets[key] = Bucket(key)\n",
    "        return res\n",
    "\n",
    "    def bucketize_PK(self, data):\n",
    "        res = copy.deepcopy(data)\n",
    "        remaining_data = np.unique(data)\n",
    "        for i, b in enumerate(self.bins):\n",
    "            res[np.isin(data, b)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, b)\n",
    "        res[np.isin(data, remaining_data)] = -1\n",
    "        return res\n",
    "\n",
    "\n",
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys\n",
    "\n",
    "\n",
    "def equal_freq_binning(name, data, n_bins, data_len, return_bucket=True):\n",
    "    uniques, counts = data\n",
    "    unique_counts, count_counts = np.unique(counts, return_counts=True)\n",
    "    idx = np.argsort(unique_counts)\n",
    "    unique_counts = unique_counts[idx]\n",
    "    count_counts = count_counts[idx]\n",
    "\n",
    "    bins = []\n",
    "    bin_modes = []\n",
    "    bin_vars = []\n",
    "    bin_means = []\n",
    "\n",
    "    bin_freq = data_len / n_bins\n",
    "    cur_freq = 0\n",
    "    cur_bin = []\n",
    "    cur_bin_count = []\n",
    "    for i, uni_c in enumerate(unique_counts):\n",
    "        cur_freq += count_counts[i] * uni_c\n",
    "        cur_bin.append(uniques[np.where(counts == uni_c)[0]])\n",
    "        cur_bin_count.extend([uni_c] * count_counts[i])\n",
    "        if (cur_freq > bin_freq) or (i == (len(unique_counts) - 1)):\n",
    "            bins.append(np.concatenate(cur_bin))\n",
    "            cur_bin_count = np.asarray(cur_bin_count)\n",
    "            bin_modes.append(uni_c)\n",
    "            bin_means.append(np.mean(cur_bin_count))\n",
    "            bin_vars.append(np.var(cur_bin_count))\n",
    "            cur_freq = 0\n",
    "            cur_bin = []\n",
    "            cur_bin_count = []\n",
    "    assert len(uniques) == sum([len(b) for b in bins]), f\"some unique values missed or duplicated\"\n",
    "    if return_bucket:\n",
    "        return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "    else:\n",
    "        return bins, bin_means\n",
    "\n",
    "\n",
    "def apply_binning_to_data(bins, bin_means, data, start_key_data, n_bins, uniques, counts):\n",
    "    # apply one greedy binning step based on existing bins\n",
    "    unique_remains = np.setdiff1d(uniques, np.concatenate(bins))\n",
    "    if len(unique_remains) != 0:\n",
    "        remaining_data = data[np.isin(data, unique_remains)]\n",
    "        unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "        unique_counts = np.unique(count_remain)\n",
    "        for u in unique_counts:\n",
    "            temp_idx = np.searchsorted(bin_means, u)\n",
    "            if temp_idx == len(bins):\n",
    "                idx = -1\n",
    "            elif temp_idx == 0:\n",
    "                idx = 0\n",
    "            else:\n",
    "                if (u - bin_means[temp_idx - 1]) >= (bin_means[temp_idx] - u):\n",
    "                    idx = temp_idx - 1\n",
    "                else:\n",
    "                    idx = temp_idx\n",
    "            temp_unique = unique_remain[count_remain == u]\n",
    "            bins[idx] = np.concatenate((bins[idx], temp_unique))   #modifying bins in place\n",
    "\n",
    "    bin_vars = []\n",
    "    temp_bin_means = []\n",
    "    for i, bin in enumerate(bins):\n",
    "        idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "        if len(idx) != 0:\n",
    "            bin_vars.append(np.var(counts[idx]))\n",
    "            temp_bin_means.append(np.mean(counts[idx]))\n",
    "        else:\n",
    "            bin_vars.append(0)\n",
    "            temp_bin_means.append(1)\n",
    "\n",
    "    assign_nbins = assign_bins_by_var(n_bins, bin_vars, temp_bin_means)\n",
    "    \n",
    "    new_bins = []\n",
    "    new_bin_means = []\n",
    "    for i, bin in enumerate(bins):\n",
    "        if assign_nbins[i] == 0:\n",
    "            new_bins.append(bin)\n",
    "            new_bin_means.append(bin_means[i])\n",
    "        else:\n",
    "            curr_bin_data = data[np.isin(data, bin)]\n",
    "            curr_start_key_data = start_key_data[np.isin(start_key_data, bin)]\n",
    "            curr_bins, curr_bin_means = divide_bin(bin, curr_bin_data, assign_nbins[i]+1, curr_start_key_data)\n",
    "            new_bins.extend(curr_bins)\n",
    "            new_bin_means.extend(curr_bin_means)\n",
    "\n",
    "    return new_bins, new_bin_means\n",
    "\n",
    "\n",
    "def assign_bins_by_var(n_bins, bin_vars, bin_means, small_threshold=0.2, large_threshold=2):\n",
    "    assign_nbins = np.zeros(len(bin_vars))\n",
    "    remaining_nbins = n_bins\n",
    "    idx = np.argsort(bin_vars)[::-1]\n",
    "    if bin_vars[idx[0]]/bin_means[idx[0]] <= small_threshold:\n",
    "        return assign_nbins\n",
    "\n",
    "    while remaining_nbins > 0:\n",
    "        for i in range(len(assign_nbins)):\n",
    "            normalized_var = bin_vars[idx[i]]/bin_means[idx[i]]\n",
    "            if normalized_var >= large_threshold:\n",
    "                assign_nbins[i] += min(remaining_nbins, 2)\n",
    "                remaining_nbins -= min(remaining_nbins, 2)\n",
    "            elif normalized_var > small_threshold:\n",
    "                assign_nbins[i] += 1\n",
    "                remaining_nbins -= 1\n",
    "            if remaining_nbins <= 0:\n",
    "                break\n",
    "    return assign_nbins\n",
    "\n",
    "\n",
    "def divide_bin(bin, curr_bin_data, n_bins, start_key_data):\n",
    "    # divide one bin into multiple bins to minimize the variance of curr_bin_data\n",
    "    uniques, counts = np.unique(curr_bin_data, return_counts=True)\n",
    "    if len(uniques) == 0:\n",
    "        return [], []\n",
    "        \n",
    "    if len(uniques) <= n_bins:\n",
    "        new_bins = []\n",
    "        bin_means = []\n",
    "        remaining_values = bin\n",
    "\n",
    "        for i, uni in enumerate(uniques):\n",
    "            new_bins.append([uni])\n",
    "            remaining_values = np.setdiff1d(remaining_values, np.asarray([uni]))\n",
    "\n",
    "        # randomly assign the remaining index to some bins\n",
    "        if len(remaining_values) > 0:\n",
    "            assign_idx = np.random.randint(0, len(new_bins), size=len(remaining_values))\n",
    "            for i in range(len(new_bins)):\n",
    "                new_bins[i].extend(list(remaining_values[assign_idx == i]))\n",
    "                new_bins[i] = np.asarray(new_bins[i])\n",
    "        \n",
    "        for bin in new_bins:\n",
    "            curr_bin_data = start_key_data[np.isin(start_key_data, bin)]\n",
    "            if len(curr_bin_data) == 0:\n",
    "                bin_means.append(0)\n",
    "            else:\n",
    "                _, count = np.unique(curr_bin_data, return_counts=True)\n",
    "                bin_means.append(np.mean(count))\n",
    "        return new_bins, bin_means\n",
    "\n",
    "    idx = np.argsort(counts)\n",
    "    counts = counts[idx]\n",
    "    uniques = uniques[idx]\n",
    "\n",
    "    # Natural breaks optimization using Fisher-Jenks Algorithms\n",
    "    breaks = jenkspy.jenks_breaks(counts, nb_class=n_bins)\n",
    "    breaks[-1] += 0.01\n",
    "    new_bins = []\n",
    "    bin_means = []\n",
    "    remaining_values = np.asarray(bin)\n",
    "    for i in range(1, len(breaks)):\n",
    "        idx = np.where((breaks[i-1] <= counts) & (counts < breaks[i]))[0]\n",
    "        new_bins.append(uniques[idx])\n",
    "        remaining_values = np.setdiff1d(remaining_values, uniques[idx])\n",
    "    \n",
    "    if len(remaining_values) > 0:\n",
    "        assign_idx = np.random.randint(0, len(new_bins), size=len(remaining_values))\n",
    "        for i in range(len(new_bins)):\n",
    "            new_bins[i] = np.concatenate((new_bins[i], remaining_values[assign_idx == i]))\n",
    "            \n",
    "    for bin in new_bins:\n",
    "        curr_bin_data = start_key_data[np.isin(start_key_data, bin)]\n",
    "        if len(curr_bin_data) == 0:\n",
    "            bin_means.append(0)\n",
    "        else:\n",
    "            _, count = np.unique(curr_bin_data, return_counts=True)\n",
    "            bin_means.append(np.mean(count))\n",
    "    return new_bins, bin_means\n",
    "\n",
    "\n",
    "def compute_variance_score(buckets):\n",
    "    \"\"\"\n",
    "    compute the variance of products of random variables\n",
    "    \"\"\"\n",
    "    all_mean = np.asarray([buckets[k].bin_means for k in buckets])\n",
    "    all_var = np.asarray([buckets[k].bin_vars for k in buckets])\n",
    "    return np.sum(np.prod(all_var + all_mean ** 2, axis=0) - np.prod(all_mean, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def greedy_bucketize(data, sample_rate, n_bins=30, primary_keys=[], return_data=False):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    A greedy algorithm that assigns half of the bins to one key at a time.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    key_orders = []\n",
    "    data_lens = []\n",
    "    curr_pk = []\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "            key_orders.append(key)\n",
    "            data_lens.append(len(data[key]))\n",
    "        else:\n",
    "            curr_pk.append(key)\n",
    "    key_orders = [key_orders[i] for i in np.argsort(data_lens)[::-1]]\n",
    "    print(key_orders)\n",
    "    print(curr_pk)\n",
    "    remaining_bins = n_bins\n",
    "    start_key = key_orders[0]\n",
    "    curr_bins = None\n",
    "    curr_bin_means = None\n",
    "    for key in key_orders:\n",
    "        print(key)\n",
    "        if key == key_orders[-1]:\n",
    "            # least key value use up all remaining bins, otherwise use half of it\n",
    "            assign_bins = remaining_bins\n",
    "        else:\n",
    "            assign_bins = remaining_bins // 2\n",
    "        if key == start_key:\n",
    "            curr_bins, curr_bin_means = equal_freq_binning(key, unique_values[key], assign_bins, len(data[key]), False)\n",
    "        else:\n",
    "            curr_bins, curr_bin_means = apply_binning_to_data(curr_bins, curr_bin_means, data[key],\n",
    "                                                              data[start_key], assign_bins,\n",
    "                                                              unique_values[key][0], unique_values[key][1])\n",
    "        print(len(curr_bins), len(curr_bin_means))\n",
    "        remaining_bins = n_bins - len(curr_bins)\n",
    "\n",
    "    new_data, best_buckets, curr_bins = bin_all_data_with_existing_binning(curr_bins, data, sample_rate, curr_pk, return_data)\n",
    "    best_buckets = Bucket_group(best_buckets, start_key, sample_rate, curr_bins, primary_keys=curr_pk)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def sub_optimal_bucketize(data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    best_variance_score = np.infty\n",
    "    best_bin_len = 0\n",
    "    best_start_key = None\n",
    "    best_buckets = None\n",
    "    for start_key in data:\n",
    "        if start_key in primary_keys:\n",
    "            continue\n",
    "        start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]), True)\n",
    "        rest_buckets = dict()\n",
    "        for key in data:\n",
    "            if key == start_key or key in primary_keys:\n",
    "                continue\n",
    "            uniques = unique_values[key][0]\n",
    "            counts = unique_values[key][1]\n",
    "            rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                       [0] * len(start_bucket.bins), uniques)\n",
    "            for i, bin in enumerate(start_bucket.bins):\n",
    "                idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "                if len(idx) != 0:\n",
    "                    bin_count = counts[idx]\n",
    "                    unique_bin_keys = uniques[idx]\n",
    "                    # unique_bin_count = np.unique(bin_count)\n",
    "                    # bin_count = np.concatenate([counts[counts == j] for j in unique_bin_count])\n",
    "                    # unique_bin_keys = np.concatenate([uniques[counts == j] for j in unique_bin_count])\n",
    "                    rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                         unique_bin_keys)\n",
    "                    rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                    rest_buckets[key].bin_vars[i] = np.var(bin_count)\n",
    "                    rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "        rest_buckets[start_key] = start_bucket\n",
    "        var_score = compute_variance_score(rest_buckets)\n",
    "        if len(start_bucket.bins) >= best_bin_len * 1.1:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "        elif len(start_bucket.bins) >= best_bin_len * 0.9 and var_score < best_variance_score:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "\n",
    "    best_buckets = Bucket_group(best_buckets, best_start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def fixed_start_key_bucketize(start_key, data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys based on the pre-defined start_key.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]), True)\n",
    "    rest_buckets = dict()\n",
    "    for key in data:\n",
    "        if key == start_key or key in primary_keys:\n",
    "            continue\n",
    "        uniques = unique_values[key][0]\n",
    "        counts = unique_values[key][1]\n",
    "        rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                   [0] * len(start_bucket.bins), uniques)\n",
    "        for i, bin in enumerate(start_bucket.bins):\n",
    "            idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "            if len(idx) != 0:\n",
    "                bin_count = counts[idx]\n",
    "                unique_bin_keys = uniques[idx]\n",
    "                rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                     unique_bin_keys)\n",
    "                rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "    best_buckets = Bucket_group(rest_buckets, start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def bin_all_data_with_existing_binning(bins, data, sample_rate, curr_pk, return_data):\n",
    "    buckets = dict()\n",
    "    new_data = dict()\n",
    "    if return_data:\n",
    "        new_data = copy.deepcopy(data)\n",
    "\n",
    "    for key in curr_pk:\n",
    "        bin_modes = [1 for i in range(len(bins))]\n",
    "        remaining_data = np.unique(data[key])\n",
    "        for i, bin in enumerate(bins):\n",
    "            if return_data:\n",
    "                new_data[key][np.isin(data[key], bin)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, bin)\n",
    "        if len(remaining_data) != 0:\n",
    "            if return_data:\n",
    "                # assigning all remaining key values to the first bin\n",
    "                new_data[key][np.isin(data[key], remaining_data)] = 0\n",
    "            bins[0] = np.concatenate((bins[0], remaining_data))\n",
    "        buckets[key] = Bucket(key, bin_modes=bin_modes)\n",
    "\n",
    "    for key in data:\n",
    "        bin_modes = []\n",
    "        for i, bin in enumerate(bins):\n",
    "            curr_data = data[key][np.isin(data[key], bin)]\n",
    "            if len(curr_data) == 0:\n",
    "                bin_modes.append(0)\n",
    "            else:\n",
    "                bin_mode = stats.mode(curr_data).count[0]\n",
    "                if bin_mode > 1:\n",
    "                    bin_mode /= sample_rate[key]\n",
    "                bin_modes.append(bin_mode)\n",
    "                if return_data:\n",
    "                    new_data[key][np.isin(data[key], bin)] = i\n",
    "        buckets[key] = Bucket(key, bin_modes=bin_modes)\n",
    "\n",
    "    return new_data, buckets, bins\n",
    "\n",
    "\n",
    "\n",
    "def apply_binning_to_data_value_count(bins, data):\n",
    "    res = np.zeros(len(bins))\n",
    "    unique_remain = np.unique(data)\n",
    "    for i, bin in enumerate(bins):\n",
    "        res[i] = np.sum(np.isin(data, bin))\n",
    "        unique_remain = np.setdiff1d(unique_remain, bin)\n",
    "\n",
    "    res[0] += np.sum(np.isin(data, unique_remain))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336bbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "schema = gen_stats_light_schema(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a531fb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'posts.Id': {'posts.Id', 'votes.PostId', 'postLinks.PostId', 'comments.PostId', 'tags.ExcerptPostId', 'postHistory.PostId', 'postLinks.RelatedPostId'}, 'users.Id': {'users.Id', 'comments.UserId', 'postHistory.UserId', 'badges.UserId', 'votes.UserId', 'posts.OwnerUserId'}}\n"
     ]
    }
   ],
   "source": [
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b86b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "badges.UserId\n",
      "0\n",
      "79851 79851\n",
      "79851 79851\n",
      "0\n",
      "votes.PostId\n",
      "0\n",
      "328064 328064\n",
      "328064 328064\n",
      "0\n",
      "votes.UserId\n",
      "293275\n",
      "328064 328064\n",
      "34773 328064\n",
      "0\n",
      "postHistory.PostId\n",
      "0\n",
      "303187 303187\n",
      "303187 303187\n",
      "0\n",
      "postHistory.UserId\n",
      "21328\n",
      "303187 303187\n",
      "277348 303187\n",
      "0\n",
      "posts.Id\n",
      "0\n",
      "91976 91976\n",
      "91976 91976\n",
      "0\n",
      "posts.OwnerUserId\n",
      "1392\n",
      "91976 91976\n",
      "90373 91976\n",
      "0\n",
      "users.Id\n",
      "0\n",
      "40325 40325\n",
      "40324 40325\n",
      "0\n",
      "comments.PostId\n",
      "0\n",
      "174305 174305\n",
      "174305 174305\n",
      "0\n",
      "comments.UserId\n",
      "2835\n",
      "174305 174305\n",
      "171470 174305\n",
      "0\n",
      "postLinks.PostId\n",
      "0\n",
      "11102 11102\n",
      "11102 11102\n",
      "0\n",
      "postLinks.RelatedPostId\n",
      "0\n",
      "11102 11102\n",
      "11102 11102\n",
      "0\n",
      "tags.ExcerptPostId\n",
      "436\n",
      "1032 1032\n",
      "596 1032\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data = dict()\n",
    "sample_rate = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    \n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            print(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr]>=0]\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92f876c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "badges.UserId 79851 0 0\n",
      "votes.PostId 328064 0 0\n",
      "votes.UserId 34773 0 0\n",
      "postHistory.PostId 303187 0 0\n",
      "postHistory.UserId 277348 0 0\n",
      "posts.Id 91976 0 0\n",
      "posts.OwnerUserId 90373 0 0\n",
      "users.Id 40324 0 0\n",
      "comments.PostId 174305 0 0\n",
      "comments.UserId 171470 0 0\n",
      "postLinks.PostId 11102 0 0\n",
      "postLinks.RelatedPostId 11102 0 0\n",
      "tags.ExcerptPostId 596 0 0\n",
      "['posts.Id', 'users.Id', 'tags.ExcerptPostId']\n"
     ]
    }
   ],
   "source": [
    "for k in data:\n",
    "    sample_rate[k] = 1.0\n",
    "    print(k, len(data[k]), np.sum(np.isnan(data[k])), len(data[k][data[k]<0]))\n",
    "print(primary_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3349d510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'posts.Id', 'votes.PostId', 'postLinks.PostId', 'comments.PostId', 'tags.ExcerptPostId', 'postHistory.PostId', 'postLinks.RelatedPostId'}\n",
      "['votes.PostId', 'postHistory.PostId', 'comments.PostId', 'postLinks.RelatedPostId', 'postLinks.PostId']\n",
      "['posts.Id', 'tags.ExcerptPostId']\n",
      "votes.PostId\n",
      "42 42\n",
      "postHistory.PostId\n",
      "171 171\n",
      "comments.PostId\n",
      "223 223\n",
      "postLinks.RelatedPostId\n",
      "240 240\n",
      "postLinks.PostId\n",
      "284 284\n",
      "{'users.Id', 'comments.UserId', 'postHistory.UserId', 'badges.UserId', 'votes.UserId', 'posts.OwnerUserId'}\n",
      "['postHistory.UserId', 'comments.UserId', 'posts.OwnerUserId', 'badges.UserId', 'votes.UserId']\n",
      "['users.Id']\n",
      "postHistory.UserId\n",
      "88 88\n",
      "comments.UserId\n",
      "194 194\n",
      "posts.OwnerUserId\n",
      "247 247\n",
      "badges.UserId\n",
      "272 272\n",
      "votes.UserId\n",
      "284 284\n"
     ]
    }
   ],
   "source": [
    "temp = dict()\n",
    "for PK in equivalent_keys:\n",
    "    print(equivalent_keys[PK])\n",
    "    group_data = {}\n",
    "    group_sample_rate = {}\n",
    "    for K in equivalent_keys[PK]:\n",
    "        group_data[K] = data[K]\n",
    "        group_sample_rate[K] = sample_rate[K]\n",
    "    d, bucket = greedy_bucketize(group_data, sample_rate, n_bins=300, primary_keys=primary_keys, return_data=True)\n",
    "    temp[PK] = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72b5001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(bucket.buckets[bucket.start_key].bins))\n",
    "for k in d:\n",
    "    print(\"     ==      \")\n",
    "    print(k, len(data[k]), np.sum(np.isnan(data[k])), len(data[k][data[k]<0]))\n",
    "    print(len(d[k]), len(np.unique(d[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a5cbedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "posts.Id 284\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "==============================================================\n",
      "votes.PostId 284\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 8.0, 8.0, 9.0, 9.0, 9.0, 9.0, 9.0, 10.0, 10.0, 10.0, 10.0, 10.0, 11.0, 11.0, 11.0, 11.0, 11.0, 12.0, 12.0, 12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 14.0, 14.0, 14.0, 15.0, 15.0, 15.0, 15.0, 15.0, 16.0, 16.0, 16.0, 17.0, 17.0, 17.0, 18.0, 18.0, 18.0, 19.0, 19.0, 19.0, 19.0, 19.0, 20.0, 20.0, 20.0, 20.0, 20.0, 21.0, 21.0, 21.0, 21.0, 21.0, 22.0, 22.0, 22.0, 24.0, 24.0, 24.0, 25.0, 25.0, 25.0, 25.0, 25.0, 27.0, 27.0, 27.0, 29.0, 29.0, 29.0, 31.0, 31.0, 31.0, 33.0, 33.0, 33.0, 33.0, 33.0, 36.0, 36.0, 36.0, 39.0, 39.0, 39.0, 43.0, 43.0, 42.0, 48.0, 48.0, 46.0, 54.0, 54.0, 54.0, 61.0, 61.0, 61.0, 69.0, 69.0, 68.0, 74.0, 78.0, 79.0, 91.0, 93.0, 92.0, 110.0, 110.0, 110.0, 131.0, 122.0, 131.0, 165.0, 155.0, 156.0, 176.0, 289.0, 254.0, 303.0, 427.0, 375.0]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "==============================================================\n",
      "postLinks.PostId 284\n",
      "[0, 1, 3.0, 0, 1, 3.0, 0, 3.0, 0, 3.0, 0, 1, 4.0, 0, 3.0, 0, 3.0, 0, 4.0, 0, 3.0, 0, 4.0, 0, 4.0, 0, 4.0, 1, 7.0, 1, 8.0, 1, 6.0, 0, 4.0, 3.0, 9.0, 0, 4.0, 1, 5.0, 1, 4.0, 2.0, 7.0, 2.0, 7.0, 0, 2.0, 1, 5.0, 0, 2.0, 0, 2.0, 1, 10.0, 0, 3.0, 0, 3.0, 2.0, 8.0, 0, 3.0, 1, 4.0, 1, 5.0, 1, 4.0, 0, 3.0, 2.0, 7.0, 3.0, 11.0, 0, 7.0, 0, 4.0, 0, 4.0, 0, 4.0, 1, 6.0, 0, 4.0, 2.0, 6.0, 1, 5.0, 1, 4.0, 1, 4.0, 0, 2.0, 1, 5.0, 3.0, 3.0, 6.0, 8.0, 7.0, 5.0, 7.0, 5.0, 4.0, 6.0, 3.0, 2.0, 3.0, 4.0, 0, 0, 0, 3.0, 4.0, 5.0, 4.0, 7.0, 5.0, 5.0, 5.0, 4.0, 0, 0, 0, 5.0, 3.0, 4.0, 3.0, 9.0, 2.0, 4.0, 8.0, 4.0, 3.0, 4.0, 4.0, 3.0, 6.0, 2.0, 0, 0, 3.0, 4.0, 4.0, 5.0, 4.0, 0, 5.0, 6.0, 7.0, 3.0, 0, 4.0, 5.0, 4.0, 5.0, 0, 4.0, 10.0, 7.0, 7.0, 0, 2.0, 4.0, 6.0, 3.0, 0, 4.0, 2.0, 5.0, 3.0, 0, 4.0, 4.0, 0, 1, 6.0, 3.0, 3.0, 0, 3.0, 4.0, 0, 2.0, 4.0, 13.0, 3.0, 1, 6.0, 8.0, 0, 3.0, 10.0, 1, 4.0, 4.0, 0, 1, 3.0, 3.0, 3.0, 0, 2.0, 7.0, 4.0, 0, 0, 3.0, 3.0, 4.0, 0, 0, 3.0, 3.0, 0, 8.0, 5.0, 0, 2.0, 2.0, 2.0, 2.0, 0, 4.0, 4.0, 0, 2.0, 4.0, 0, 2.0, 3.0, 0, 4.0, 4.0, 3.0, 1, 1, 2.0, 3.0, 0, 5.0, 5.0, 6.0, 4.0, 1, 2.0, 5.0, 4.0, 0, 5.0, 2.0, 0, 3.0, 5.0, 0, 4.0, 2.0, 0, 2.0, 2.0, 1, 1, 1, 0, 6.0, 5.0, 2.0, 3.0, 2.0, 2.0, 2.0, 0, 2.0, 2.0, 2.0, 2.0, 0, 4.0]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "==============================================================\n",
      "comments.PostId 284\n",
      "[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 14.0, 15.0, 30.0, 18.0, 12.0, 25.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 12.0, 24.0, 21.0, 22.0, 16.0, 24.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 13.0, 19.0, 17.0, 23.0, 13.0, 20.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 13.0, 11.0, 12.0, 15.0, 32.0, 14.0, 1, 1, 5.0, 5.0, 17.0, 16.0, 2.0, 2.0, 6.0, 6.0, 21.0, 28.0, 2.0, 2.0, 6.0, 6.0, 18.0, 16.0, 3.0, 3.0, 8.0, 10.0, 31.0, 25.0, 2.0, 2.0, 6.0, 19.0, 2.0, 6.0, 33.0, 2.0, 6.0, 29.0, 2.0, 7.0, 17.0, 3.0, 8.0, 20.0, 1, 5.0, 24.0, 2.0, 6.0, 27.0, 2.0, 6.0, 23.0, 2.0, 7.0, 22.0, 2.0, 7.0, 30.0, 2.0, 6.0, 31.0, 2.0, 7.0, 20.0, 2.0, 6.0, 22.0, 3.0, 9.0, 21.0, 5.0, 14.0, 33.0, 0, 18.0, 19.0, 19.0, 19.0, 22.0, 17.0, 24.0, 34.0, 41.0, 29.0, 19.0, 14.0, 24.0, 41.0, 24.0, 24.0, 16.0, 19.0, 20.0, 22.0, 24.0, 13.0, 17.0, 28.0, 19.0, 15.0, 9.0, 20.0, 45.0, 28.0, 14.0, 14.0, 17.0, 20.0, 11.0, 18.0, 18.0, 20.0, 37.0, 15.0, 19.0, 15.0, 10.0, 17.0, 19.0, 19.0, 33.0, 15.0, 12.0, 11.0, 14.0, 17.0, 35.0, 18.0, 16.0, 11.0, 6.0, 11.0, 16.0, 11.0, 13.0, 9.0, 24.0, 9.0, 12.0, 11.0, 16.0, 8.0, 16.0, 11.0, 10.0, 17.0, 19.0, 14.0, 10.0, 13.0, 17.0, 16.0, 15.0, 16.0, 20.0, 6.0, 9.0, 14.0, 17.0, 9.0, 8.0, 14.0, 37.0, 15.0, 9.0, 8.0, 12.0, 14.0, 11.0, 6.0, 17.0, 18.0, 17.0, 10.0, 12.0, 14.0, 12.0, 10.0, 10.0, 8.0, 30.0, 16.0, 13.0, 9.0, 14.0, 11.0, 17.0, 8.0, 10.0, 9.0, 10.0, 30.0, 12.0, 10.0, 22.0, 8.0, 5.0, 15.0, 8.0, 17.0, 24.0, 5.0, 14.0, 9.0, 8.0, 23.0, 6.0, 11.0, 35.0, 7.0, 3.0, 10.0]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "==============================================================\n",
      "tags.ExcerptPostId 284\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "==============================================================\n",
      "postHistory.PostId 284\n",
      "[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 12.0, 11.0, 15.0, 12.0, 13.0, 10.0, 15.0, 14.0, 18.0, 14.0, 14.0, 13.0, 13.0, 11.0, 13.0, 15.0, 15.0, 13.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 13.0, 15.0, 16.0, 17.0, 15.0, 15.0, 3.0, 3.0, 3.0, 3.0, 5.0, 5.0, 5.0, 7.0, 7.0, 7.0, 10.0, 10.0, 10.0, 20.0, 23.0, 21.0, 1, 1, 1, 3.0, 3.0, 3.0, 6.0, 6.0, 6.0, 20.0, 22.0, 27.0, 2.0, 2.0, 2.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 7.0, 7.0, 7.0, 10.0, 10.0, 10.0, 21.0, 22.0, 16.0, 0, 1, 3.0, 5.0, 7.0, 10.0, 21.0, 1, 3.0, 5.0, 9.0, 30.0, 1, 3.0, 6.0, 10.0, 20.0, 1, 3.0, 6.0, 10.0, 19.0, 1, 4.0, 7.0, 11.0, 24.0, 1, 3.0, 5.0, 8.0, 19.0, 2.0, 6.0, 21.0, 1, 3.0, 6.0, 9.0, 18.0, 2.0, 6.0, 21.0, 1, 4.0, 7.0, 12.0, 23.0, 3.0, 7.0, 19.0, 2.0, 6.0, 17.0, 3.0, 8.0, 18.0, 1, 3.0, 6.0, 11.0, 18.0, 2.0, 5.0, 8.0, 13.0, 29.0, 2.0, 4.0, 7.0, 11.0, 16.0, 2.0, 6.0, 15.0, 3.0, 8.0, 21.0, 2.0, 5.0, 9.0, 14.0, 30.0, 2.0, 6.0, 14.0, 2.0, 6.0, 19.0, 2.0, 6.0, 15.0, 2.0, 4.0, 6.0, 9.0, 14.0, 3.0, 8.0, 19.0, 2.0, 6.0, 15.0, 4.0, 11.0, 20.0, 4.0, 12.0, 24.0, 2.0, 6.0, 11.0, 3.0, 7.0, 15.0, 3.0, 7.0, 13.0, 2.0, 5.0, 10.0, 4.0, 9.0, 32.0, 2.0, 6.0, 14.0, 4.0, 7.0, 11.0, 6.0, 12.0, 22.0, 7.0, 10.0, 16.0, 5.0, 9.0, 15.0]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "==============================================================\n",
      "postLinks.RelatedPostId 284\n",
      "[0, 0, 0, 1, 1, 1, 2.0, 4.0, 0, 0, 0, 0, 0, 2.0, 2.0, 0, 0, 1, 1, 3.0, 3.0, 0, 0, 1, 1, 4.0, 2.0, 0, 0, 2.0, 2.0, 7.0, 0, 0, 0, 0, 0, 2.0, 2.0, 0, 0, 0, 0, 2.0, 2.0, 0, 0, 1, 1, 0, 3.0, 0, 0, 1, 1, 2.0, 6.0, 0, 0, 1, 2.0, 3.0, 6.0, 0, 0, 1, 1, 0, 4.0, 0, 0, 0, 0, 1, 1, 4.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0, 2.0, 1, 1, 1, 2.0, 2.0, 1, 1, 4.0, 5.0, 4.0, 2.0, 4.0, 10.0, 2.0, 6.0, 4.0, 2.0, 4.0, 3.0, 10.0, 3.0, 2.0, 1, 0, 0, 0, 2.0, 3.0, 3.0, 10.0, 5.0, 4.0, 5.0, 5.0, 11.0, 0, 0, 0, 6.0, 4.0, 4.0, 4.0, 3.0, 3.0, 6.0, 5.0, 2.0, 2.0, 3.0, 3.0, 3.0, 2.0, 0, 0, 0, 4.0, 7.0, 5.0, 5.0, 2.0, 1, 7.0, 7.0, 9.0, 7.0, 0, 4.0, 7.0, 11.0, 7.0, 0, 3.0, 21.0, 16.0, 3.0, 0, 4.0, 5.0, 7.0, 7.0, 0, 3.0, 9.0, 6.0, 4.0, 0, 19.0, 18.0, 0, 8.0, 19.0, 6.0, 5.0, 0, 10.0, 8.0, 0, 11.0, 9.0, 4.0, 7.0, 2.0, 14.0, 14.0, 0, 13.0, 6.0, 5.0, 19.0, 12.0, 0, 5.0, 5.0, 17.0, 0, 0, 9.0, 22.0, 16.0, 4.0, 0, 10.0, 7.0, 16.0, 10.0, 0, 11.0, 11.0, 1, 18.0, 8.0, 0, 6.0, 12.0, 15.0, 6.0, 0, 8.0, 18.0, 0, 15.0, 6.0, 0, 8.0, 26.0, 1, 5.0, 11.0, 12.0, 15.0, 5.0, 19.0, 27.0, 0, 37.0, 15.0, 20.0, 12.0, 2.0, 4.0, 24.0, 13.0, 0, 22.0, 11.0, 4.0, 25.0, 57.0, 0, 20.0, 10.0, 0, 16.0, 55.0, 15.0, 39.0, 1, 0, 37.0, 31.0, 93.0, 33.0, 39.0, 29.0, 30.0, 0, 96.0, 36.0, 59.0, 13.0, 11.0, 45.0]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "bucket = temp[\"posts.Id\"]\n",
    "for k in bucket.buckets:\n",
    "    print(\"==============================================================\")\n",
    "    print(k, len(bucket.buckets[k].bin_modes))\n",
    "    print(bucket.buckets[k].bin_modes)\n",
    "    print([len(b) for b in bucket.buckets[k].bins])\n",
    "    print(bucket.buckets[k].bin_means)\n",
    "    print(bucket.buckets[k].bin_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32563005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/Users/ziniuw/Desktop/research/Learned_QO/CE_scheme/\")\n",
    "\n",
    "from Schemas.stats.schema import gen_stats_light_schema\n",
    "from Join_scheme.binning import identify_key_values, sub_optimal_bucketize, greedy_bucketize, Table_bucket\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def timestamp_transorform(time_string, start_date=\"2010-07-19 00:00:00\"):\n",
    "    start_date_int = time.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    time_array = time.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return int(time.mktime(time_array)) - int(time.mktime(start_date_int))\n",
    "\n",
    "\n",
    "def read_table_hdf(table_obj):\n",
    "    \"\"\"\n",
    "    Reads hdf from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    df_rows = pd.read_hdf(table_obj.csv_file_location)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def convert_time_to_int(data_folder):\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_file_location = data_folder + file\n",
    "            df_rows = pd.read_csv(csv_file_location)\n",
    "            for attribute in df_rows.columns:\n",
    "                if \"Date\" in attribute:\n",
    "                    if df_rows[attribute].values.dtype == 'object':\n",
    "                        new_value = []\n",
    "                        for value in df_rows[attribute].values:\n",
    "                            new_value.append(timestamp_transorform(value))\n",
    "                        df_rows[attribute] = new_value\n",
    "            df_rows.to_csv(csv_file_location, index=False)\n",
    "\n",
    "\n",
    "def read_table_csv(table_obj, csv_seperator=',', stats=True):\n",
    "    \"\"\"\n",
    "    Reads csv from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    if stats:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    else:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                              quotechar='\"',\n",
    "                              sep=csv_seperator)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def generate_table_buckets(data, key_attrs, bin_sizes, bin_modes, optimal_buckets):\n",
    "    table_buckets = dict()\n",
    "    for table in data:\n",
    "        table_data = data[table]\n",
    "        table_bucket = Table_bucket(table, key_attrs[table], bin_sizes[table])\n",
    "        for key in key_attrs[table]:\n",
    "            if key in bin_modes and len(bin_modes[key]) != 0:\n",
    "                table_bucket.oned_bin_modes[key] = bin_modes[key]\n",
    "            else:\n",
    "                # this is a primary key\n",
    "                table_bucket.oned_bin_modes[key] = np.ones(table_bucket.bin_sizes[key])\n",
    "        # getting mode for 2D bins\n",
    "        if len(key_attrs[table]) == 2:\n",
    "            key1 = key_attrs[table][0]\n",
    "            key2 = key_attrs[table][1]\n",
    "            res1 = np.zeros((table_bucket.bin_sizes[key1], table_bucket.bin_sizes[key2]))\n",
    "            res2 = np.zeros((table_bucket.bin_sizes[key1], table_bucket.bin_sizes[key2]))\n",
    "            key_data = np.stack((table_data[key1].values, table_data[key2].values), axis=1)\n",
    "            assert table_bucket.bin_sizes[key1] == len(optimal_buckets[key1].bins)\n",
    "            assert table_bucket.bin_sizes[key2] == len(optimal_buckets[key2].bins)\n",
    "            for v1, b1 in enumerate(optimal_buckets[key1].bins):\n",
    "                temp_data = key_data[np.isin(key_data[:, 0], b1)]\n",
    "                if len(temp_data) == 0:\n",
    "                    continue\n",
    "                assert np.max(np.unique(temp_data[:, 0], return_counts=True)[-1]) == table_bucket.oned_bin_modes[key1][\n",
    "                    v1], f\"{key1} data error at {v1}, with \" \\\n",
    "                         f\"{np.max(np.unique(temp_data[:, 0], return_counts=True)[-1])} and \" \\\n",
    "                         f\"{table_bucket.oned_bin_modes[key1][v1]}.\"\n",
    "                for v2, b2 in enumerate(optimal_buckets[key2].bins):\n",
    "                    temp_data2 = copy.deepcopy(temp_data[np.isin(temp_data[:, 1], b2)])\n",
    "                    if len(temp_data2) == 0:\n",
    "                        continue\n",
    "                    res1[v1, v2] = np.max(np.unique(temp_data2[:, 0], return_counts=True)[-1])\n",
    "                    res2[v1, v2] = np.max(np.unique(temp_data2[:, 1], return_counts=True)[-1])\n",
    "            table_bucket.twod_bin_modes[key1] = res1\n",
    "            table_bucket.twod_bin_modes[key2] = res2\n",
    "        table_buckets[table] = table_bucket\n",
    "\n",
    "    return table_buckets\n",
    "\n",
    "\n",
    "def process_stats_data(data_path, model_folder, n_bins=500, bucket_method=\"greedy\", save_bucket_bins=False):\n",
    "    \"\"\"\n",
    "    Preprocessing stats data and generate optimal bucket\n",
    "    :param data_path: path to stats data folder\n",
    "    :param n_bins: number of bins (the actually number of bins returned will be smaller than this)\n",
    "    :param bucket_method: choose between \"sub_optimal\" and \"greedy\". Please refer to binning.py for details.\n",
    "    :param save_bucket_bins: Set to true for dynamic environment, the default is False for static environment\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not data_path.endswith(\".csv\"):\n",
    "        data_path += \"/{}.csv\"\n",
    "    schema = gen_stats_light_schema(data_path)\n",
    "    all_keys, equivalent_keys = identify_key_values(schema)\n",
    "    data = dict()\n",
    "    key_data = dict()  # store the columns of all keys\n",
    "    sample_rate = dict()\n",
    "    primary_keys = []\n",
    "    null_values = dict()\n",
    "    key_attrs = dict()\n",
    "    for table_obj in schema.tables:\n",
    "        table_name = table_obj.table_name\n",
    "        null_values[table_name] = dict()\n",
    "        key_attrs[table_name] = []\n",
    "        df_rows = read_table_csv(table_obj, stats=True)\n",
    "        for attr in df_rows.columns:\n",
    "            if attr in all_keys:\n",
    "                key_data[attr] = df_rows[attr].values\n",
    "                # the nan value of id are set to -1, this is hardcoded.\n",
    "                key_data[attr][np.isnan(key_data[attr])] = -1\n",
    "                key_data[attr][key_data[attr] < 0] = -1\n",
    "                null_values[table_name][attr] = -1\n",
    "                key_data[attr] = copy.deepcopy(key_data[attr])[key_data[attr] >= 0]\n",
    "                # if the all keys have exactly one appearance, we consider them primary keys\n",
    "                # we set a error margin of 0.01 in case of data mis-write.\n",
    "                if len(np.unique(key_data[attr])) >= len(key_data[attr]) * 0.99:\n",
    "                    primary_keys.append(attr)\n",
    "                sample_rate[attr] = 1.0\n",
    "                key_attrs[table_name].append(attr)\n",
    "            else:\n",
    "                temp = df_rows[attr].values\n",
    "                null_values[table_name][attr] = np.nanmin(temp) - 100\n",
    "                temp[np.isnan(temp)] = null_values[table_name][attr]\n",
    "        data[table_name] = df_rows\n",
    "\n",
    "    all_bin_modes = dict()\n",
    "    bin_size = dict()\n",
    "    binned_data = dict()\n",
    "    optimal_buckets = dict()\n",
    "    for PK in equivalent_keys:\n",
    "        print(f\"bucketizing equivalent key group:\", equivalent_keys[PK])\n",
    "        group_data = {}\n",
    "        group_sample_rate = {}\n",
    "        for K in equivalent_keys[PK]:\n",
    "            group_data[K] = key_data[K]\n",
    "            group_sample_rate[K] = sample_rate[K]\n",
    "        if bucket_method == \"greedy\":\n",
    "            temp_data, optimal_bucket = greedy_bucketize(group_data, sample_rate, n_bins, primary_keys, True)\n",
    "        elif bucket_method == \"sub_optimal\":\n",
    "            temp_data, optimal_bucket = sub_optimal_bucketize(group_data, sample_rate, n_bins, primary_keys, True)\n",
    "        else:\n",
    "            assert False, f\"unrecognized bucketization method: {bucket_method}\"\n",
    "\n",
    "        binned_data.update(temp_data)\n",
    "        for K in equivalent_keys[PK]:\n",
    "            optimal_buckets[K] = optimal_bucket\n",
    "            temp_table_name = K.split(\".\")[0]\n",
    "            if temp_table_name not in bin_size:\n",
    "                bin_size[temp_table_name] = dict()\n",
    "            bin_size[temp_table_name][K] = len(optimal_bucket.bins)\n",
    "            all_bin_modes[K] = np.asarray(optimal_bucket.buckets[K].bin_modes)\n",
    "\n",
    "    table_buckets = generate_table_buckets(data, key_attrs, bin_size, all_bin_modes, optimal_buckets)\n",
    "\n",
    "    for K in binned_data:\n",
    "        temp_table_name = K.split(\".\")[0]\n",
    "        temp = data[temp_table_name][K].values\n",
    "        temp[temp >= 0] = binned_data[K]\n",
    "\n",
    "    if save_bucket_bins:\n",
    "        with open(model_folder + f\"/buckets.pkl\") as f:\n",
    "            pickle.dump(optimal_buckets, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "49973d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketizing equivalent key group: {'posts.Id', 'votes.PostId', 'postLinks.PostId', 'comments.PostId', 'tags.ExcerptPostId', 'postHistory.PostId', 'postLinks.RelatedPostId'}\n",
      "['votes.PostId', 'postHistory.PostId', 'comments.PostId', 'postLinks.RelatedPostId', 'postLinks.PostId']\n",
      "['posts.Id', 'tags.ExcerptPostId']\n",
      "votes.PostId\n",
      "34 34\n",
      "postHistory.PostId\n",
      "117 117\n",
      "comments.PostId\n",
      "155 155\n",
      "postLinks.RelatedPostId\n",
      "168 168\n",
      "postLinks.PostId\n",
      "190 190\n",
      "bucketizing equivalent key group: {'users.Id', 'comments.UserId', 'postHistory.UserId', 'badges.UserId', 'votes.UserId', 'posts.OwnerUserId'}\n",
      "['postHistory.UserId', 'comments.UserId', 'posts.OwnerUserId', 'badges.UserId', 'votes.UserId']\n",
      "['users.Id']\n",
      "postHistory.UserId\n",
      "66 66\n",
      "comments.UserId\n",
      "133 133\n",
      "posts.OwnerUserId\n",
      "166 166\n",
      "badges.UserId\n",
      "183 183\n",
      "votes.UserId\n",
      "197 197\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/ubuntu/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "model_folder = \"/home/ubuntu/data_CE/saved_models\"\n",
    "data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size = process_stats_data(data_path,\n",
    "                                                                       model_folder, 200, \"greedy\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "718ed6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>70747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>70747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>70747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>70747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>70747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Id  UserId   Date\n",
       "0           0   1       5  70747\n",
       "1           1   2       6  70747\n",
       "2           2   3       8  70747\n",
       "3           3   4      23  70747\n",
       "4           4   5      36  70747"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rows = pd.read_csv(data_path.format(\"badges\"))\n",
    "df_rows.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6868f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"votes\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"postLinks\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba052cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket.oned_bin_modes[bucket.id_attributes[0]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[0]], axis=1))\n",
    "print(bucket.oned_bin_modes[bucket.id_attributes[1]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d89f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
