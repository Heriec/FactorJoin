{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/CE_scheme/\")\n",
    "from Schemas.stats.schema import gen_stats_light_schema\n",
    "from Join_scheme.data_prepare import read_table_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51dec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy import stats\n",
    "import jenkspy\n",
    "\n",
    "\n",
    "class Bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization of a key attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bins=[], bin_modes=[], bin_vars=[], bin_means=[], rest_bins_remaining=None):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        self.bin_modes = bin_modes\n",
    "        self.bin_vars = bin_vars\n",
    "        self.bin_means = bin_means\n",
    "        self.rest_bins_remaining = rest_bins_remaining\n",
    "        if len(bins) != 0:\n",
    "            assert len(bins) == len(bin_modes)\n",
    "\n",
    "\n",
    "class Table_bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization for all key attributes in a table.\n",
    "    Supporting more than three dimensional bin modes requires simplifying the causal structure, which is left as a\n",
    "    future work.\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name, id_attributes, bin_sizes):\n",
    "        self.table_name = table_name\n",
    "        self.id_attributes = id_attributes\n",
    "        self.bin_sizes = bin_sizes\n",
    "        self.oned_bin_modes = dict()\n",
    "        self.twod_bin_modes = dict()\n",
    "\n",
    "\n",
    "class Bucket_group:\n",
    "    \"\"\"\n",
    "    The class of bucketization for a group of equivalent join keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buckets, start_key, sample_rate, bins=None, primary_keys=[]):\n",
    "        self.buckets = buckets\n",
    "        self.start_key = start_key\n",
    "        self.sample_rate = sample_rate\n",
    "        self.bins = bins\n",
    "        self.primary_keys = primary_keys\n",
    "\n",
    "    def bucketize(self, data):\n",
    "        \"\"\"\n",
    "        Discretize data based on the bucket\n",
    "        \"\"\"\n",
    "        res = dict()\n",
    "        seen_remain_key = np.array([])\n",
    "        cumulative_bin = copy.deepcopy(self.buckets[self.start_key].bins)\n",
    "        start_means = np.asarray(self.buckets[self.start_key].bin_means)\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            if key != self.start_key:\n",
    "                unique_remain = np.setdiff1d(self.buckets[key].rest_bins_remaining, seen_remain_key)\n",
    "                assert sum([np.sum(np.isin(unique_remain, b) == 1) for b in cumulative_bin]) == 0\n",
    "\n",
    "                if len(unique_remain) != 0:\n",
    "                    remaining_data = data[key][np.isin(data[key], unique_remain)]\n",
    "                    unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "                    unique_counts = np.unique(count_remain)\n",
    "                    for u in unique_counts:\n",
    "                        temp_idx = np.searchsorted(start_means, u)\n",
    "                        if temp_idx == len(cumulative_bin):\n",
    "                            idx = -1\n",
    "                            if u > self.buckets[key].bin_modes[-1]:\n",
    "                                self.buckets[key].bin_modes[-1] = u\n",
    "                        elif temp_idx == 0:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            if (u - start_means[temp_idx - 1]) >= (start_means[temp_idx] - u):\n",
    "                                idx = temp_idx - 1\n",
    "                            else:\n",
    "                                idx = temp_idx\n",
    "                        temp_unique = unique_remain[count_remain == u]\n",
    "                        cumulative_bin[idx] = np.concatenate((cumulative_bin[idx], temp_unique))\n",
    "                        seen_remain_key = np.concatenate((seen_remain_key, temp_unique))\n",
    "                        if u > self.buckets[key].bin_modes[idx]:\n",
    "                            self.buckets[key].bin_modes[idx] = u\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            count = 0\n",
    "            for i, b in enumerate(cumulative_bin):\n",
    "                count += len(data[key][np.isin(data[key], b)])\n",
    "                res[key][np.isin(data[key], b)] = i\n",
    "        self.bins = cumulative_bin\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                res[key] = self.bucketize_PK(data[key])\n",
    "                self.buckets[key] = Bucket(key)\n",
    "        return res\n",
    "\n",
    "    def bucketize_PK(self, data):\n",
    "        res = copy.deepcopy(data)\n",
    "        remaining_data = np.unique(data)\n",
    "        for i, b in enumerate(self.bins):\n",
    "            res[np.isin(data, b)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, b)\n",
    "        res[np.isin(data, remaining_data)] = -1\n",
    "        return res\n",
    "\n",
    "\n",
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys\n",
    "\n",
    "\n",
    "def equal_freq_binning(name, data, n_bins, data_len, return_bucket=True):\n",
    "    uniques, counts = data\n",
    "    unique_counts, count_counts = np.unique(counts, return_counts=True)\n",
    "    idx = np.argsort(unique_counts)\n",
    "    unique_counts = unique_counts[idx]\n",
    "    count_counts = count_counts[idx]\n",
    "\n",
    "    bins = []\n",
    "    bin_modes = []\n",
    "    bin_vars = []\n",
    "    bin_means = []\n",
    "\n",
    "    bin_freq = data_len / n_bins\n",
    "    cur_freq = 0\n",
    "    cur_bin = []\n",
    "    cur_bin_count = []\n",
    "    for i, uni_c in enumerate(unique_counts):\n",
    "        cur_freq += count_counts[i] * uni_c\n",
    "        cur_bin.append(uniques[np.where(counts == uni_c)[0]])\n",
    "        cur_bin_count.extend([uni_c] * count_counts[i])\n",
    "        if (cur_freq > bin_freq) or (i == (len(unique_counts) - 1)):\n",
    "            bins.append(np.concatenate(cur_bin))\n",
    "            cur_bin_count = np.asarray(cur_bin_count)\n",
    "            bin_modes.append(uni_c)\n",
    "            bin_means.append(np.mean(cur_bin_count))\n",
    "            bin_vars.append(np.var(cur_bin_count))\n",
    "            cur_freq = 0\n",
    "            cur_bin = []\n",
    "            cur_bin_count = []\n",
    "    assert len(uniques) == sum([len(b) for b in bins]), f\"some unique values missed or duplicated\"\n",
    "    if return_bucket:\n",
    "        return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "    else:\n",
    "        return bins, bin_means\n",
    "\n",
    "\n",
    "def apply_binning_to_data(bins, bin_means, data, start_key_data, n_bins, uniques, counts):\n",
    "    # apply one greedy binning step based on existing bins\n",
    "    unique_remains = np.setdiff1d(uniques, np.concatenate(bins))\n",
    "    if len(unique_remains) != 0:\n",
    "        remaining_data = data[np.isin(data, unique_remains)]\n",
    "        unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "        unique_counts = np.unique(count_remain)\n",
    "        for u in unique_counts:\n",
    "            temp_idx = np.searchsorted(bin_means, u)\n",
    "            if temp_idx == len(bins):\n",
    "                idx = -1\n",
    "            elif temp_idx == 0:\n",
    "                idx = 0\n",
    "            else:\n",
    "                if (u - bin_means[temp_idx - 1]) >= (bin_means[temp_idx] - u):\n",
    "                    idx = temp_idx - 1\n",
    "                else:\n",
    "                    idx = temp_idx\n",
    "            temp_unique = unique_remain[count_remain == u]\n",
    "            bins[idx] = np.concatenate((bins[idx], temp_unique))   #modifying bins in place\n",
    "\n",
    "    bin_vars = []\n",
    "    temp_bin_means = []\n",
    "    for i, bin in enumerate(bins):\n",
    "        idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "        if len(idx) != 0:\n",
    "            bin_vars.append(np.var(counts[idx]))\n",
    "            temp_bin_means.append(np.mean(counts[idx]))\n",
    "        else:\n",
    "            bin_vars.append(0)\n",
    "            temp_bin_means.append(1)\n",
    "\n",
    "    assign_nbins = assign_bins_by_var(n_bins, bin_vars, temp_bin_means)\n",
    "    \n",
    "    new_bins = []\n",
    "    new_bin_means = []\n",
    "    for i, bin in enumerate(bins):\n",
    "        if assign_nbins[i] == 0:\n",
    "            new_bins.append(bin)\n",
    "            new_bin_means.append(bin_means[i])\n",
    "        else:\n",
    "            curr_bin_data = data[np.isin(data, bin)]\n",
    "            curr_start_key_data = start_key_data[np.isin(start_key_data, bin)]\n",
    "            curr_bins, curr_bin_means = divide_bin(bin, curr_bin_data, assign_nbins[i]+1, curr_start_key_data)\n",
    "            new_bins.extend(curr_bins)\n",
    "            new_bin_means.extend(curr_bin_means)\n",
    "\n",
    "    return new_bins, new_bin_means\n",
    "\n",
    "\n",
    "def assign_bins_by_var(n_bins, bin_vars, bin_means, small_threshold=0.2, large_threshold=2):\n",
    "    assign_nbins = np.zeros(len(bin_vars))\n",
    "    remaining_nbins = n_bins\n",
    "    idx = np.argsort(bin_vars)[::-1]\n",
    "    if bin_vars[idx[0]]/bin_means[idx[0]] <= small_threshold:\n",
    "        return assign_nbins\n",
    "\n",
    "    while remaining_nbins > 0:\n",
    "        for i in range(len(assign_nbins)):\n",
    "            normalized_var = bin_vars[idx[i]]/bin_means[idx[i]]\n",
    "            if normalized_var >= large_threshold:\n",
    "                assign_nbins[i] += min(remaining_nbins, 2)\n",
    "                remaining_nbins -= min(remaining_nbins, 2)\n",
    "            elif normalized_var > small_threshold:\n",
    "                assign_nbins[i] += 1\n",
    "                remaining_nbins -= 1\n",
    "            if remaining_nbins <= 0:\n",
    "                break\n",
    "    return assign_nbins\n",
    "\n",
    "\n",
    "def divide_bin(bin, curr_bin_data, n_bins, start_key_data):\n",
    "    # divide one bin into multiple bins to minimize the variance of curr_bin_data\n",
    "    uniques, counts = np.unique(curr_bin_data, return_counts=True)\n",
    "    if len(uniques) == 0:\n",
    "        return [], []\n",
    "        \n",
    "    if len(uniques) <= n_bins:\n",
    "        new_bins = []\n",
    "        bin_means = []\n",
    "        remaining_values = bin\n",
    "\n",
    "        for i, uni in enumerate(uniques):\n",
    "            new_bins.append([uni])\n",
    "            remaining_values = np.setdiff1d(remaining_values, np.asarray([uni]))\n",
    "\n",
    "        # randomly assign the remaining index to some bins\n",
    "        if len(remaining_values) > 0:\n",
    "            assign_idx = np.random.randint(0, len(new_bins), size=len(remaining_values))\n",
    "            for i in range(len(new_bins)):\n",
    "                new_bins[i].extend(list(remaining_values[assign_idx == i]))\n",
    "                new_bins[i] = np.asarray(new_bins[i])\n",
    "        \n",
    "        for bin in new_bins:\n",
    "            curr_bin_data = start_key_data[np.isin(start_key_data, bin)]\n",
    "            if len(curr_bin_data) == 0:\n",
    "                bin_means.append(0)\n",
    "            else:\n",
    "                _, count = np.unique(curr_bin_data, return_counts=True)\n",
    "                bin_means.append(np.mean(count))\n",
    "        return new_bins, bin_means\n",
    "\n",
    "    idx = np.argsort(counts)\n",
    "    counts = counts[idx]\n",
    "    uniques = uniques[idx]\n",
    "\n",
    "    # Natural breaks optimization using Fisher-Jenks Algorithms\n",
    "    breaks = jenkspy.jenks_breaks(counts, nb_class=n_bins)\n",
    "    breaks[-1] += 0.01\n",
    "    new_bins = []\n",
    "    bin_means = []\n",
    "    remaining_values = np.asarray(bin)\n",
    "    for i in range(1, len(breaks)):\n",
    "        idx = np.where((breaks[i-1] <= counts) & (counts < breaks[i]))[0]\n",
    "        new_bins.append(uniques[idx])\n",
    "        remaining_values = np.setdiff1d(remaining_values, uniques[idx])\n",
    "    \n",
    "    if len(remaining_values) > 0:\n",
    "        assign_idx = np.random.randint(0, len(new_bins), size=len(remaining_values))\n",
    "        for i in range(len(new_bins)):\n",
    "            new_bins[i] = np.concatenate((new_bins[i], remaining_values[assign_idx == i]))\n",
    "            \n",
    "    for bin in new_bins:\n",
    "        curr_bin_data = start_key_data[np.isin(start_key_data, bin)]\n",
    "        if len(curr_bin_data) == 0:\n",
    "            bin_means.append(0)\n",
    "        else:\n",
    "            _, count = np.unique(curr_bin_data, return_counts=True)\n",
    "            bin_means.append(np.mean(count))\n",
    "    return new_bins, bin_means\n",
    "\n",
    "\n",
    "def compute_variance_score(buckets):\n",
    "    \"\"\"\n",
    "    compute the variance of products of random variables\n",
    "    \"\"\"\n",
    "    all_mean = np.asarray([buckets[k].bin_means for k in buckets])\n",
    "    all_var = np.asarray([buckets[k].bin_vars for k in buckets])\n",
    "    return np.sum(np.prod(all_var + all_mean ** 2, axis=0) - np.prod(all_mean, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def greedy_bucketize(data, sample_rate, n_bins=30, primary_keys=[], return_data=False):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    A greedy algorithm that assigns half of the bins to one key at a time.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    key_orders = []\n",
    "    data_lens = []\n",
    "    curr_pk = []\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "            key_orders.append(key)\n",
    "            data_lens.append(len(data[key]))\n",
    "        else:\n",
    "            curr_pk.append(key)\n",
    "    key_orders = [key_orders[i] for i in np.argsort(data_lens)[::-1]]\n",
    "    print(key_orders)\n",
    "    print(curr_pk)\n",
    "    remaining_bins = n_bins\n",
    "    start_key = key_orders[0]\n",
    "    curr_bins = None\n",
    "    curr_bin_means = None\n",
    "    for key in key_orders:\n",
    "        print(key)\n",
    "        if key == key_orders[-1]:\n",
    "            # least key value use up all remaining bins, otherwise use half of it\n",
    "            assign_bins = remaining_bins\n",
    "        else:\n",
    "            assign_bins = remaining_bins // 2\n",
    "        if key == start_key:\n",
    "            curr_bins, curr_bin_means = equal_freq_binning(key, unique_values[key], assign_bins, len(data[key]), False)\n",
    "        else:\n",
    "            curr_bins, curr_bin_means = apply_binning_to_data(curr_bins, curr_bin_means, data[key],\n",
    "                                                              data[start_key], assign_bins,\n",
    "                                                              unique_values[key][0], unique_values[key][1])\n",
    "        print(len(curr_bins), len(curr_bin_means))\n",
    "        remaining_bins = n_bins - len(curr_bins)\n",
    "\n",
    "    new_data, best_buckets, curr_bins = bin_all_data_with_existing_binning(curr_bins, data, sample_rate, curr_pk, return_data)\n",
    "    best_buckets = Bucket_group(best_buckets, start_key, sample_rate, curr_bins, primary_keys=curr_pk)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def sub_optimal_bucketize(data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    best_variance_score = np.infty\n",
    "    best_bin_len = 0\n",
    "    best_start_key = None\n",
    "    best_buckets = None\n",
    "    for start_key in data:\n",
    "        if start_key in primary_keys:\n",
    "            continue\n",
    "        start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]), True)\n",
    "        rest_buckets = dict()\n",
    "        for key in data:\n",
    "            if key == start_key or key in primary_keys:\n",
    "                continue\n",
    "            uniques = unique_values[key][0]\n",
    "            counts = unique_values[key][1]\n",
    "            rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                       [0] * len(start_bucket.bins), uniques)\n",
    "            for i, bin in enumerate(start_bucket.bins):\n",
    "                idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "                if len(idx) != 0:\n",
    "                    bin_count = counts[idx]\n",
    "                    unique_bin_keys = uniques[idx]\n",
    "                    # unique_bin_count = np.unique(bin_count)\n",
    "                    # bin_count = np.concatenate([counts[counts == j] for j in unique_bin_count])\n",
    "                    # unique_bin_keys = np.concatenate([uniques[counts == j] for j in unique_bin_count])\n",
    "                    rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                         unique_bin_keys)\n",
    "                    rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                    rest_buckets[key].bin_vars[i] = np.var(bin_count)\n",
    "                    rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "        rest_buckets[start_key] = start_bucket\n",
    "        var_score = compute_variance_score(rest_buckets)\n",
    "        if len(start_bucket.bins) >= best_bin_len * 1.1:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "        elif len(start_bucket.bins) >= best_bin_len * 0.9 and var_score < best_variance_score:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "\n",
    "    best_buckets = Bucket_group(best_buckets, best_start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def fixed_start_key_bucketize(start_key, data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys based on the pre-defined start_key.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]), True)\n",
    "    rest_buckets = dict()\n",
    "    for key in data:\n",
    "        if key == start_key or key in primary_keys:\n",
    "            continue\n",
    "        uniques = unique_values[key][0]\n",
    "        counts = unique_values[key][1]\n",
    "        rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                   [0] * len(start_bucket.bins), uniques)\n",
    "        for i, bin in enumerate(start_bucket.bins):\n",
    "            idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "            if len(idx) != 0:\n",
    "                bin_count = counts[idx]\n",
    "                unique_bin_keys = uniques[idx]\n",
    "                rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                     unique_bin_keys)\n",
    "                rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "    best_buckets = Bucket_group(rest_buckets, start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def bin_all_data_with_existing_binning(bins, data, sample_rate, curr_pk, return_data):\n",
    "    buckets = dict()\n",
    "    new_data = dict()\n",
    "    if return_data:\n",
    "        new_data = copy.deepcopy(data)\n",
    "\n",
    "    for key in curr_pk:\n",
    "        bin_modes = [1 for i in range(len(bins))]\n",
    "        remaining_data = np.unique(data[key])\n",
    "        for i, bin in enumerate(bins):\n",
    "            if return_data:\n",
    "                new_data[key][np.isin(data[key], bin)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, bin)\n",
    "        if len(remaining_data) != 0:\n",
    "            if return_data:\n",
    "                # assigning all remaining key values to the first bin\n",
    "                new_data[key][np.isin(data[key], remaining_data)] = 0\n",
    "            bins[0] = np.concatenate((bins[0], remaining_data))\n",
    "        buckets[key] = Bucket(key, bin_modes=bin_modes)\n",
    "\n",
    "    for key in data:\n",
    "        bin_modes = []\n",
    "        for i, bin in enumerate(bins):\n",
    "            curr_data = data[key][np.isin(data[key], bin)]\n",
    "            if len(curr_data) == 0:\n",
    "                bin_modes.append(0)\n",
    "            else:\n",
    "                bin_mode = stats.mode(curr_data).count[0]\n",
    "                if bin_mode > 1:\n",
    "                    bin_mode /= sample_rate[key]\n",
    "                bin_modes.append(bin_mode)\n",
    "                if return_data:\n",
    "                    new_data[key][np.isin(data[key], bin)] = i\n",
    "        buckets[key] = Bucket(key, bin_modes=bin_modes)\n",
    "\n",
    "    return new_data, buckets, bins\n",
    "\n",
    "\n",
    "\n",
    "def apply_binning_to_data_value_count(bins, data):\n",
    "    res = np.zeros(len(bins))\n",
    "    unique_remain = np.unique(data)\n",
    "    for i, bin in enumerate(bins):\n",
    "        res[i] = np.sum(np.isin(data, bin))\n",
    "        unique_remain = np.setdiff1d(unique_remain, bin)\n",
    "\n",
    "    res[0] += np.sum(np.isin(data, unique_remain))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "schema = gen_stats_light_schema(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "sample_rate = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    \n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            print(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr]>=0]\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f876c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data:\n",
    "    sample_rate[k] = 1.0\n",
    "    print(k, len(data[k]), np.sum(np.isnan(data[k])), len(data[k][data[k]<0]))\n",
    "print(primary_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict()\n",
    "for PK in equivalent_keys:\n",
    "    print(equivalent_keys[PK])\n",
    "    group_data = {}\n",
    "    group_sample_rate = {}\n",
    "    for K in equivalent_keys[PK]:\n",
    "        group_data[K] = data[K]\n",
    "        group_sample_rate[K] = sample_rate[K]\n",
    "    d, bucket = greedy_bucketize(group_data, sample_rate, n_bins=300, primary_keys=primary_keys, return_data=True)\n",
    "    temp[PK] = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bucket.buckets[bucket.start_key].bins))\n",
    "for k in d:\n",
    "    print(\"     ==      \")\n",
    "    print(k, len(data[k]), np.sum(np.isnan(data[k])), len(data[k][data[k]<0]))\n",
    "    print(len(d[k]), len(np.unique(d[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = temp[\"posts.Id\"]\n",
    "for k in bucket.buckets:\n",
    "    print(\"==============================================================\")\n",
    "    print(k, len(bucket.buckets[k].bin_modes))\n",
    "    print(bucket.buckets[k].bin_modes)\n",
    "    print([len(b) for b in bucket.buckets[k].bins])\n",
    "    print(bucket.buckets[k].bin_means)\n",
    "    print(bucket.buckets[k].bin_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32563005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/Users/ziniuw/Desktop/research/Learned_QO/CE_scheme/\")\n",
    "\n",
    "from Schemas.stats.schema import gen_stats_light_schema\n",
    "from Join_scheme.binning import identify_key_values, sub_optimal_bucketize, greedy_bucketize, Table_bucket\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def timestamp_transorform(time_string, start_date=\"2010-07-19 00:00:00\"):\n",
    "    start_date_int = time.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    time_array = time.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return int(time.mktime(time_array)) - int(time.mktime(start_date_int))\n",
    "\n",
    "\n",
    "def read_table_hdf(table_obj):\n",
    "    \"\"\"\n",
    "    Reads hdf from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    df_rows = pd.read_hdf(table_obj.csv_file_location)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def convert_time_to_int(data_folder):\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_file_location = data_folder + file\n",
    "            df_rows = pd.read_csv(csv_file_location)\n",
    "            for attribute in df_rows.columns:\n",
    "                if \"Date\" in attribute:\n",
    "                    if df_rows[attribute].values.dtype == 'object':\n",
    "                        new_value = []\n",
    "                        for value in df_rows[attribute].values:\n",
    "                            new_value.append(timestamp_transorform(value))\n",
    "                        df_rows[attribute] = new_value\n",
    "            df_rows.to_csv(csv_file_location, index=False)\n",
    "\n",
    "\n",
    "def read_table_csv(table_obj, csv_seperator=',', stats=True):\n",
    "    \"\"\"\n",
    "    Reads csv from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    if stats:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    else:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                              quotechar='\"',\n",
    "                              sep=csv_seperator)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def generate_table_buckets(data, key_attrs, bin_sizes, bin_modes, optimal_buckets):\n",
    "    table_buckets = dict()\n",
    "    for table in data:\n",
    "        table_data = data[table]\n",
    "        table_bucket = Table_bucket(table, key_attrs[table], bin_sizes[table])\n",
    "        for key in key_attrs[table]:\n",
    "            if key in bin_modes and len(bin_modes[key]) != 0:\n",
    "                table_bucket.oned_bin_modes[key] = bin_modes[key]\n",
    "            else:\n",
    "                # this is a primary key\n",
    "                table_bucket.oned_bin_modes[key] = np.ones(table_bucket.bin_sizes[key])\n",
    "        # getting mode for 2D bins\n",
    "        if len(key_attrs[table]) == 2:\n",
    "            key1 = key_attrs[table][0]\n",
    "            key2 = key_attrs[table][1]\n",
    "            res1 = np.zeros((table_bucket.bin_sizes[key1], table_bucket.bin_sizes[key2]))\n",
    "            res2 = np.zeros((table_bucket.bin_sizes[key1], table_bucket.bin_sizes[key2]))\n",
    "            key_data = np.stack((table_data[key1].values, table_data[key2].values), axis=1)\n",
    "            assert table_bucket.bin_sizes[key1] == len(optimal_buckets[key1].bins)\n",
    "            assert table_bucket.bin_sizes[key2] == len(optimal_buckets[key2].bins)\n",
    "            for v1, b1 in enumerate(optimal_buckets[key1].bins):\n",
    "                temp_data = key_data[np.isin(key_data[:, 0], b1)]\n",
    "                if len(temp_data) == 0:\n",
    "                    continue\n",
    "                #print(key1, v1, np.max(np.unique(temp_data[:, 0], return_counts=True)[-1]), table_bucket.oned_bin_modes[key1][v1])\n",
    "                #assert np.max(np.unique(temp_data[:, 0], return_counts=True)[-1]) == table_bucket.oned_bin_modes[key1][\n",
    "                    #v1], f\"{key1} data error at {v1}, with \" \\\n",
    "                     #    f\"{np.max(np.unique(temp_data[:, 0], return_counts=True)[-1])} and \" \\\n",
    "                      #   f\"{table_bucket.oned_bin_modes[key1][v1]}.\"\n",
    "                for v2, b2 in enumerate(optimal_buckets[key2].bins):\n",
    "                    temp_data2 = copy.deepcopy(temp_data[np.isin(temp_data[:, 1], b2)])\n",
    "                    if len(temp_data2) == 0:\n",
    "                        continue\n",
    "                    res1[v1, v2] = np.max(np.unique(temp_data2[:, 0], return_counts=True)[-1])\n",
    "                    res2[v1, v2] = np.max(np.unique(temp_data2[:, 1], return_counts=True)[-1])\n",
    "            table_bucket.twod_bin_modes[key1] = res1\n",
    "            table_bucket.twod_bin_modes[key2] = res2\n",
    "        table_buckets[table] = table_bucket\n",
    "\n",
    "    return table_buckets\n",
    "\n",
    "\n",
    "def process_stats_data(data_path, model_folder, n_bins=500, bucket_method=\"greedy\", save_bucket_bins=False):\n",
    "    \"\"\"\n",
    "    Preprocessing stats data and generate optimal bucket\n",
    "    :param data_path: path to stats data folder\n",
    "    :param n_bins: number of bins (the actually number of bins returned will be smaller than this)\n",
    "    :param bucket_method: choose between \"sub_optimal\" and \"greedy\". Please refer to binning.py for details.\n",
    "    :param save_bucket_bins: Set to true for dynamic environment, the default is False for static environment\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not data_path.endswith(\".csv\"):\n",
    "        data_path += \"/{}.csv\"\n",
    "    schema = gen_stats_light_schema(data_path)\n",
    "    all_keys, equivalent_keys = identify_key_values(schema)\n",
    "    data = dict()\n",
    "    key_data = dict()  # store the columns of all keys\n",
    "    sample_rate = dict()\n",
    "    primary_keys = []\n",
    "    null_values = dict()\n",
    "    key_attrs = dict()\n",
    "    for table_obj in schema.tables:\n",
    "        table_name = table_obj.table_name\n",
    "        null_values[table_name] = dict()\n",
    "        key_attrs[table_name] = []\n",
    "        df_rows = read_table_csv(table_obj, stats=True)\n",
    "        for attr in df_rows.columns:\n",
    "            if attr in all_keys:\n",
    "                key_data[attr] = df_rows[attr].values\n",
    "                # the nan value of id are set to -1, this is hardcoded.\n",
    "                key_data[attr][np.isnan(key_data[attr])] = -1\n",
    "                key_data[attr][key_data[attr] < 0] = -1\n",
    "                null_values[table_name][attr] = -1\n",
    "                key_data[attr] = copy.deepcopy(key_data[attr])[key_data[attr] >= 0]\n",
    "                # if the all keys have exactly one appearance, we consider them primary keys\n",
    "                # we set a error margin of 0.01 in case of data mis-write.\n",
    "                if len(np.unique(key_data[attr])) >= len(key_data[attr]) * 0.99:\n",
    "                    primary_keys.append(attr)\n",
    "                sample_rate[attr] = 1.0\n",
    "                key_attrs[table_name].append(attr)\n",
    "            else:\n",
    "                temp = df_rows[attr].values\n",
    "                null_values[table_name][attr] = np.nanmin(temp) - 100\n",
    "                temp[np.isnan(temp)] = null_values[table_name][attr]\n",
    "        data[table_name] = df_rows\n",
    "\n",
    "    all_bin_modes = dict()\n",
    "    bin_size = dict()\n",
    "    binned_data = dict()\n",
    "    optimal_buckets = dict()\n",
    "    for PK in equivalent_keys:\n",
    "        print(f\"bucketizing equivalent key group:\", equivalent_keys[PK])\n",
    "        group_data = {}\n",
    "        group_sample_rate = {}\n",
    "        for K in equivalent_keys[PK]:\n",
    "            group_data[K] = key_data[K]\n",
    "            group_sample_rate[K] = sample_rate[K]\n",
    "        if bucket_method == \"greedy\":\n",
    "            temp_data, optimal_bucket = greedy_bucketize(group_data, sample_rate, n_bins, primary_keys, True)\n",
    "        elif bucket_method == \"sub_optimal\":\n",
    "            temp_data, optimal_bucket = sub_optimal_bucketize(group_data, sample_rate, n_bins, primary_keys)\n",
    "        else:\n",
    "            assert False, f\"unrecognized bucketization method: {bucket_method}\"\n",
    "\n",
    "        binned_data.update(temp_data)\n",
    "        for K in equivalent_keys[PK]:\n",
    "            optimal_buckets[K] = optimal_bucket\n",
    "            temp_table_name = K.split(\".\")[0]\n",
    "            if temp_table_name not in bin_size:\n",
    "                bin_size[temp_table_name] = dict()\n",
    "            bin_size[temp_table_name][K] = len(optimal_bucket.bins)\n",
    "            all_bin_modes[K] = np.asarray(optimal_bucket.buckets[K].bin_modes)\n",
    "\n",
    "    table_buckets = generate_table_buckets(data, key_attrs, bin_size, all_bin_modes, optimal_buckets)\n",
    "\n",
    "    for K in binned_data:\n",
    "        temp_table_name = K.split(\".\")[0]\n",
    "        temp = data[temp_table_name][K].values\n",
    "        temp[temp >= 0] = binned_data[K]\n",
    "\n",
    "    if save_bucket_bins:\n",
    "        with open(model_folder + f\"/buckets.pkl\") as f:\n",
    "            pickle.dump(optimal_buckets, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49973d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "model_folder = \"/home/ubuntu/data_CE/saved_models\"\n",
    "data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size = process_stats_data(data_path,\n",
    "                                                                       model_folder, 200, \"sub_optimal\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ed6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = pd.read_csv(data_path.format(\"badges\"))\n",
    "df_rows.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6868f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"votes\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"postLinks\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba052cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket.oned_bin_modes[bucket.id_attributes[0]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[0]], axis=1))\n",
    "print(bucket.oned_bin_modes[bucket.id_attributes[1]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d89f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
