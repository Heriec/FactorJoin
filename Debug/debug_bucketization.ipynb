{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv, process_stats_data, process_imdb_data\n",
    "from Join_scheme.bound import Bound_ensemble\n",
    "from Join_scheme.join_graph import parse_query_all_join, get_join_hyper_graph\n",
    "from Evaluation.testing import get_job_sub_plan_queires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = {\n",
    "            'title.id': 800,\n",
    "            'info_type.id': 100,\n",
    "            'keyword.id': 100,\n",
    "            'company_name.id': 100,\n",
    "            'name.id': 100,\n",
    "            'company_type.id': 100,\n",
    "            'comp_cast_type.id': 50,\n",
    "            'kind_type.id': 50,\n",
    "            'char_name.id': 50,\n",
    "            'role_type.id': 50,\n",
    "            'link_type.id': 50\n",
    "        }\n",
    "data_path = \"/Users/ziniuw/Desktop/research/Learned_QO/data/imdb/{}.csv\"\n",
    "model_folder = \"/Users/ziniuw/Desktop/research/Learned_QO/data/saved_models\"\n",
    "schema, table_buckets, ground_truth_factors_no_filter = process_imdb_data(data_path, model_folder, n_bins, \"fixed_start_key\",\n",
    "                                                                              save_bucket_bins=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ef015",
   "metadata": {},
   "outputs": [],
   "source": [
    "be = Bound_ensemble(table_buckets, schema, 1, ground_truth_factors_no_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = model_folder + f\"model_imdb_default.pkl\"\n",
    "pickle.dump(be, open(model_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c84f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"/Users/ziniuw/Desktop/research/Learned_QO/data/saved_models\"\n",
    "model_path = model_folder + f\"model_imdb_default.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    be = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d7b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "be.SPERCENTAGE = 1.0\n",
    "be.query_sample_location = \"../Sampling/materialized_IMDB_JOB_sample/{}/job/all_job/\"\n",
    "query_folder = \"../IMDB-JOB/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4728cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries, all_sub_plan_queries_str = get_job_sub_plan_queires(query_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90c0b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 27 is out of bounds for axis 0 with size 27",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-def88e3c39d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"19b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cardinality_bound_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_queries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sub_plan_queries_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/research/Learned_QO/FactorJoin/Join_scheme/bound.py\u001b[0m in \u001b[0;36mget_cardinality_bound_all\u001b[0;34m(self, query_str, sub_plan_query_str_all, query_name, debug, true_card)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mconditional_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_id_conidtional_distribution_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequivalent_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mconditional_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_id_conidtional_distribution_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;31m# self.reverse_table_alias = {v: k for k, v in tables_all.items()}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mcached_sub_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/Learned_QO/FactorJoin/Join_scheme/bound.py\u001b[0m in \u001b[0;36mget_all_id_conidtional_distribution_sample\u001b[0;34m(self, query_file_name, tables_alias, join_keys)\u001b[0m\n\u001b[1;32m    142\u001b[0m             return load_sample_imdb_one_query(self.table_buckets, tables_alias, query_file_name, join_keys,\n\u001b[1;32m    143\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mground_truth_factors_no_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPERCENTAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                                               self.query_sample_location)\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# TODO: sample on the fly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/Learned_QO/FactorJoin/Sampling/load_sample.py\u001b[0m in \u001b[0;36mload_sample_imdb_one_query\u001b[0;34m(table_buckets, tables_alias, query_file_name, join_keys, table_key_equivalent_group, SPERCENTAGE, qdir)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mpdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mtable_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtable_len\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 27 is out of bounds for axis 0 with size 27"
     ]
    }
   ],
   "source": [
    "q_name = \"19b\"\n",
    "temp = be.get_cardinality_bound_all(all_queries[q_name], all_sub_plan_queries_str[q_name], q_name + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "schema = gen_stats_light_schema(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "sample_rate = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    \n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            print(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr]>=0]\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f876c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data:\n",
    "    sample_rate[k] = 1.0\n",
    "    print(k, len(data[k]), np.sum(np.isnan(data[k])), len(data[k][data[k]<0]))\n",
    "print(primary_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict()\n",
    "for PK in equivalent_keys:\n",
    "    print(equivalent_keys[PK])\n",
    "    group_data = {}\n",
    "    group_sample_rate = {}\n",
    "    for K in equivalent_keys[PK]:\n",
    "        group_data[K] = data[K]\n",
    "        group_sample_rate[K] = sample_rate[K]\n",
    "    d, bucket = naive_bucketize(group_data, sample_rate, n_bins=100, primary_keys=primary_keys, return_data=True)\n",
    "    temp[PK] = bucket\n",
    "    for key in d:\n",
    "        print(np.min(d[key]), np.max(d[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bucket.buckets[bucket.start_key].bins))\n",
    "for k in d:\n",
    "    print(\"     ==      \")\n",
    "    print(k, len(data[k]), np.sum(np.isnan(data[k])), len(data[k][data[k]<0]))\n",
    "    print(len(d[k]), len(np.unique(d[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = temp[\"posts.Id\"]\n",
    "for k in bucket.buckets:\n",
    "    print(\"==============================================================\")\n",
    "    print(k, len(bucket.buckets[k].bin_modes))\n",
    "    print(bucket.buckets[k].bin_modes)\n",
    "    print([len(b) for b in bucket.buckets[k].bins])\n",
    "    print(bucket.buckets[k].bin_means)\n",
    "    print(bucket.buckets[k].bin_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32563005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/Users/ziniuw/Desktop/research/Learned_QO/CE_scheme/\")\n",
    "\n",
    "from Schemas.stats.schema import gen_stats_light_schema\n",
    "from Join_scheme.binning import identify_key_values, sub_optimal_bucketize, greedy_bucketize, Table_bucket\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def timestamp_transorform(time_string, start_date=\"2010-07-19 00:00:00\"):\n",
    "    start_date_int = time.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    time_array = time.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return int(time.mktime(time_array)) - int(time.mktime(start_date_int))\n",
    "\n",
    "\n",
    "def read_table_hdf(table_obj):\n",
    "    \"\"\"\n",
    "    Reads hdf from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    df_rows = pd.read_hdf(table_obj.csv_file_location)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def convert_time_to_int(data_folder):\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_file_location = data_folder + file\n",
    "            df_rows = pd.read_csv(csv_file_location)\n",
    "            for attribute in df_rows.columns:\n",
    "                if \"Date\" in attribute:\n",
    "                    if df_rows[attribute].values.dtype == 'object':\n",
    "                        new_value = []\n",
    "                        for value in df_rows[attribute].values:\n",
    "                            new_value.append(timestamp_transorform(value))\n",
    "                        df_rows[attribute] = new_value\n",
    "            df_rows.to_csv(csv_file_location, index=False)\n",
    "\n",
    "\n",
    "def read_table_csv(table_obj, csv_seperator=',', stats=True):\n",
    "    \"\"\"\n",
    "    Reads csv from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    if stats:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    else:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                              quotechar='\"',\n",
    "                              sep=csv_seperator)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def generate_table_buckets(data, key_attrs, bin_sizes, bin_modes, optimal_buckets):\n",
    "    table_buckets = dict()\n",
    "    for table in data:\n",
    "        table_data = data[table]\n",
    "        table_bucket = Table_bucket(table, key_attrs[table], bin_sizes[table])\n",
    "        for key in key_attrs[table]:\n",
    "            if key in bin_modes and len(bin_modes[key]) != 0:\n",
    "                table_bucket.oned_bin_modes[key] = bin_modes[key]\n",
    "            else:\n",
    "                # this is a primary key\n",
    "                table_bucket.oned_bin_modes[key] = np.ones(table_bucket.bin_sizes[key])\n",
    "        # getting mode for 2D bins\n",
    "        if len(key_attrs[table]) == 2:\n",
    "            key1 = key_attrs[table][0]\n",
    "            key2 = key_attrs[table][1]\n",
    "            res1 = np.zeros((table_bucket.bin_sizes[key1], table_bucket.bin_sizes[key2]))\n",
    "            res2 = np.zeros((table_bucket.bin_sizes[key1], table_bucket.bin_sizes[key2]))\n",
    "            key_data = np.stack((table_data[key1].values, table_data[key2].values), axis=1)\n",
    "            assert table_bucket.bin_sizes[key1] == len(optimal_buckets[key1].bins)\n",
    "            assert table_bucket.bin_sizes[key2] == len(optimal_buckets[key2].bins)\n",
    "            for v1, b1 in enumerate(optimal_buckets[key1].bins):\n",
    "                temp_data = key_data[np.isin(key_data[:, 0], b1)]\n",
    "                if len(temp_data) == 0:\n",
    "                    continue\n",
    "                #print(key1, v1, np.max(np.unique(temp_data[:, 0], return_counts=True)[-1]), table_bucket.oned_bin_modes[key1][v1])\n",
    "                #assert np.max(np.unique(temp_data[:, 0], return_counts=True)[-1]) == table_bucket.oned_bin_modes[key1][\n",
    "                    #v1], f\"{key1} data error at {v1}, with \" \\\n",
    "                     #    f\"{np.max(np.unique(temp_data[:, 0], return_counts=True)[-1])} and \" \\\n",
    "                      #   f\"{table_bucket.oned_bin_modes[key1][v1]}.\"\n",
    "                for v2, b2 in enumerate(optimal_buckets[key2].bins):\n",
    "                    temp_data2 = copy.deepcopy(temp_data[np.isin(temp_data[:, 1], b2)])\n",
    "                    if len(temp_data2) == 0:\n",
    "                        continue\n",
    "                    res1[v1, v2] = np.max(np.unique(temp_data2[:, 0], return_counts=True)[-1])\n",
    "                    res2[v1, v2] = np.max(np.unique(temp_data2[:, 1], return_counts=True)[-1])\n",
    "            table_bucket.twod_bin_modes[key1] = res1\n",
    "            table_bucket.twod_bin_modes[key2] = res2\n",
    "        table_buckets[table] = table_bucket\n",
    "\n",
    "    return table_buckets\n",
    "\n",
    "\n",
    "def process_stats_data(data_path, model_folder, n_bins=500, bucket_method=\"greedy\", save_bucket_bins=False):\n",
    "    \"\"\"\n",
    "    Preprocessing stats data and generate optimal bucket\n",
    "    :param data_path: path to stats data folder\n",
    "    :param n_bins: number of bins (the actually number of bins returned will be smaller than this)\n",
    "    :param bucket_method: choose between \"sub_optimal\" and \"greedy\". Please refer to binning.py for details.\n",
    "    :param save_bucket_bins: Set to true for dynamic environment, the default is False for static environment\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not data_path.endswith(\".csv\"):\n",
    "        data_path += \"/{}.csv\"\n",
    "    schema = gen_stats_light_schema(data_path)\n",
    "    all_keys, equivalent_keys = identify_key_values(schema)\n",
    "    data = dict()\n",
    "    key_data = dict()  # store the columns of all keys\n",
    "    sample_rate = dict()\n",
    "    primary_keys = []\n",
    "    null_values = dict()\n",
    "    key_attrs = dict()\n",
    "    for table_obj in schema.tables:\n",
    "        table_name = table_obj.table_name\n",
    "        null_values[table_name] = dict()\n",
    "        key_attrs[table_name] = []\n",
    "        df_rows = read_table_csv(table_obj, stats=True)\n",
    "        for attr in df_rows.columns:\n",
    "            if attr in all_keys:\n",
    "                key_data[attr] = df_rows[attr].values\n",
    "                # the nan value of id are set to -1, this is hardcoded.\n",
    "                key_data[attr][np.isnan(key_data[attr])] = -1\n",
    "                key_data[attr][key_data[attr] < 0] = -1\n",
    "                null_values[table_name][attr] = -1\n",
    "                key_data[attr] = copy.deepcopy(key_data[attr])[key_data[attr] >= 0]\n",
    "                # if the all keys have exactly one appearance, we consider them primary keys\n",
    "                # we set a error margin of 0.01 in case of data mis-write.\n",
    "                if len(np.unique(key_data[attr])) >= len(key_data[attr]) * 0.99:\n",
    "                    primary_keys.append(attr)\n",
    "                sample_rate[attr] = 1.0\n",
    "                key_attrs[table_name].append(attr)\n",
    "            else:\n",
    "                temp = df_rows[attr].values\n",
    "                null_values[table_name][attr] = np.nanmin(temp) - 100\n",
    "                temp[np.isnan(temp)] = null_values[table_name][attr]\n",
    "        data[table_name] = df_rows\n",
    "\n",
    "    all_bin_modes = dict()\n",
    "    bin_size = dict()\n",
    "    binned_data = dict()\n",
    "    optimal_buckets = dict()\n",
    "    for PK in equivalent_keys:\n",
    "        print(f\"bucketizing equivalent key group:\", equivalent_keys[PK])\n",
    "        group_data = {}\n",
    "        group_sample_rate = {}\n",
    "        for K in equivalent_keys[PK]:\n",
    "            group_data[K] = key_data[K]\n",
    "            group_sample_rate[K] = sample_rate[K]\n",
    "        if bucket_method == \"greedy\":\n",
    "            temp_data, optimal_bucket = greedy_bucketize(group_data, sample_rate, n_bins, primary_keys, True)\n",
    "        elif bucket_method == \"sub_optimal\":\n",
    "            temp_data, optimal_bucket = sub_optimal_bucketize(group_data, sample_rate, n_bins, primary_keys)\n",
    "        else:\n",
    "            assert False, f\"unrecognized bucketization method: {bucket_method}\"\n",
    "\n",
    "        binned_data.update(temp_data)\n",
    "        for K in equivalent_keys[PK]:\n",
    "            optimal_buckets[K] = optimal_bucket\n",
    "            temp_table_name = K.split(\".\")[0]\n",
    "            if temp_table_name not in bin_size:\n",
    "                bin_size[temp_table_name] = dict()\n",
    "            bin_size[temp_table_name][K] = len(optimal_bucket.bins)\n",
    "            all_bin_modes[K] = np.asarray(optimal_bucket.buckets[K].bin_modes)\n",
    "\n",
    "    table_buckets = generate_table_buckets(data, key_attrs, bin_size, all_bin_modes, optimal_buckets)\n",
    "\n",
    "    for K in binned_data:\n",
    "        temp_table_name = K.split(\".\")[0]\n",
    "        temp = data[temp_table_name][K].values\n",
    "        temp[temp >= 0] = binned_data[K]\n",
    "\n",
    "    if save_bucket_bins:\n",
    "        with open(model_folder + f\"/buckets.pkl\") as f:\n",
    "            pickle.dump(optimal_buckets, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49973d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "model_folder = \"/home/ubuntu/data_CE/saved_models\"\n",
    "data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size = process_stats_data(data_path,\n",
    "                                                                       model_folder, 200, \"sub_optimal\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ed6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = pd.read_csv(data_path.format(\"badges\"))\n",
    "df_rows.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6868f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"votes\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b92583",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.twod_bin_modes[attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"postHistory\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba052cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket.oned_bin_modes[bucket.id_attributes[0]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[0]], axis=1))\n",
    "print(bucket.oned_bin_modes[bucket.id_attributes[1]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d89f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bins = np.histogram(a, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ffbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[np.where((a<30) & (a>15))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66498d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa29ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
