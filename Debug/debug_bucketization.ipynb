{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv, process_stats_data, process_imdb_data\n",
    "from Join_scheme.bound import Bound_ensemble\n",
    "from Join_scheme.join_graph import parse_query_all_join, get_join_hyper_graph\n",
    "from Evaluation.testing import get_job_sub_plan_queires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b9d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziniuw/opt/anaconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3361: DtypeWarning: Columns (5,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/Users/ziniuw/opt/anaconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3361: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/Users/ziniuw/opt/anaconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3361: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/Users/ziniuw/opt/anaconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3361: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title.kind_id\n",
      "movie_info.info_type_id\n",
      "movie_keyword.movie_id\n",
      "cast_info.person_id\n",
      "cast_info.person_role_id\n",
      "cast_info.role_id\n",
      "complete_cast.status_id\n",
      "movie_keyword.keyword_id\n",
      "movie_companies.company_id\n",
      "movie_companies.company_type_id\n",
      "kind_type.id 8\n",
      "info_type.id 72\n",
      "title.id 74\n",
      "name.id 27\n",
      "char_name.id 28\n",
      "role_type.id 12\n",
      "comp_cast_type.id 2\n",
      "keyword.id 86\n",
      "company_name.id 75\n",
      "company_type.id 3\n"
     ]
    }
   ],
   "source": [
    "n_bins = {\n",
    "            'title.id': 800,\n",
    "            'info_type.id': 100,\n",
    "            'keyword.id': 100,\n",
    "            'company_name.id': 100,\n",
    "            'name.id': 100,\n",
    "            'company_type.id': 100,\n",
    "            'comp_cast_type.id': 50,\n",
    "            'kind_type.id': 50,\n",
    "            'char_name.id': 50,\n",
    "            'role_type.id': 50,\n",
    "            'link_type.id': 50\n",
    "        }\n",
    "data_path = \"/Users/ziniuw/Desktop/research/Learned_QO/data/imdb/{}.csv\"\n",
    "model_folder = \"/Users/ziniuw/Desktop/research/Learned_QO/data/saved_models\"\n",
    "schema, table_buckets, ground_truth_factors_no_filter = process_imdb_data(data_path, model_folder, n_bins, \"fixed_start_key\",\n",
    "                                                                              save_bucket_bins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de113b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from Sampling.utils.query_storage import load_sql_rep, save_sql_rep\n",
    "from Sampling.utils.parse_sql import parse_sql\n",
    "import collections.abc\n",
    "import sqlparse\n",
    "#hyper needs the four following aliases to be done manually.\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "collections.Mapping = collections.abc.Mapping\n",
    "collections.MutableSet = collections.abc.MutableSet\n",
    "collections.MutableMapping = collections.abc.MutableMapping\n",
    "\n",
    "fn = \"/Users/ziniuw/Desktop/research/Learned_QO/factorjoin-binned-cards/queries/job/all_job/17a.pkl\"\n",
    "qrep = load_sql_rep(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6e26dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ci', 'cn', 'k', 'mc', 'mk', 'n', 't'), ('ci', 'cn', 'k', 'mc', 'mk', 't'), ('ci', 'cn', 'mc', 'mk', 'n', 't'), ('ci', 'k', 'mc', 'mk', 'n', 't'), ('ci', 'cn', 'k', 'mc', 'mk', 'n'), ('ci', 'k', 'mc', 'mk', 't'), ('cn', 'k', 'mc', 'mk', 't'), ('ci', 'k', 'mk', 'n', 't'), ('ci', 'cn', 'mc', 'n', 't'), ('ci', 'cn', 'mc', 'mk', 'n'), ('ci', 'cn', 'k', 'mc', 'mk'), ('ci', 'k', 'mc', 'mk', 'n'), ('ci', 'cn', 'mc', 'mk', 't'), ('ci', 'mc', 'mk', 'n', 't'), ('k', 'mc', 'mk', 't'), ('ci', 'cn', 'mc', 'n'), ('ci', 'mc', 'n', 't'), ('ci', 'mc', 'mk', 'n'), ('ci', 'cn', 'mc', 't'), ('ci', 'k', 'mk', 'n'), ('cn', 'k', 'mc', 'mk'), ('ci', 'k', 'mk', 't'), ('ci', 'mc', 'mk', 't'), ('ci', 'mk', 'n', 't'), ('ci', 'cn', 'mc', 'mk'), ('ci', 'k', 'mc', 'mk'), ('cn', 'mc', 'mk', 't'), ('cn', 'mc', 't'), ('mc', 'mk', 't'), ('ci', 'mc', 'n'), ('cn', 'mc', 'mk'), ('ci', 'n', 't'), ('k', 'mk', 't'), ('ci', 'cn', 'mc'), ('ci', 'k', 'mk'), ('k', 'mc', 'mk'), ('ci', 'mk', 't'), ('ci', 'mc', 't'), ('ci', 'mk', 'n'), ('ci', 'mc', 'mk'), ('ci', 'n'), ('mk', 't'), ('mc', 'mk'), ('ci', 't'), ('cn', 'mc'), ('ci', 'mc'), ('mc', 't'), ('ci', 'mk'), ('k', 'mk'), ('k',), ('ci',), ('mc',), ('cn',), ('t',), ('n',), ('mk',)]\n"
     ]
    }
   ],
   "source": [
    "node_list = list(qrep[\"subset_graph\"].nodes())\n",
    "node_list.sort(reverse=True, key=lambda x: len(x))\n",
    "print(node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a637a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " movie_keyword\n"
     ]
    }
   ],
   "source": [
    "sg = qrep[\"join_graph\"].subgraph(('mk',))\n",
    "tname = qrep[\"join_graph\"].nodes()['mk'][\"real_name\"]\n",
    "print(sg, tname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f24e52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = sqlparse.parse(qrep['sql'])[0]\n",
    "from_clause = None\n",
    "from_seen = False\n",
    "where_clauses = None\n",
    "for token in parsed.tokens:\n",
    "    if isinstance(token, sqlparse.sql.Where):\n",
    "        where_clauses = token\n",
    "for token in parsed.tokens:\n",
    "    if from_seen:\n",
    "        if isinstance(token, sqlparse.sql.IdentifierList) or isinstance(token,\n",
    "                sqlparse.sql.Identifier):\n",
    "            from_clause = token\n",
    "            break\n",
    "    if token.value.upper() == 'FROM':\n",
    "        from_seen = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "128a94a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cast_info AS ci,\n",
      "     company_name AS cn,\n",
      "     keyword AS k,\n",
      "     movie_companies AS mc,\n",
      "     movie_keyword AS mk,\n",
      "     name AS n,\n",
      "     title AS t\n"
     ]
    }
   ],
   "source": [
    "print(from_clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d0cb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "\n",
    "\n",
    "def find_next_match(tables, wheres, index):\n",
    "    '''\n",
    "    ignore everything till next\n",
    "    '''\n",
    "    match = \"\"\n",
    "    _, token = wheres.token_next(index)\n",
    "    if token is None:\n",
    "        return None, None\n",
    "    # FIXME: is this right?\n",
    "    if token.is_keyword:\n",
    "        index, token = wheres.token_next(index)\n",
    "\n",
    "    tables_in_pred = find_all_tables_till_keyword(token)\n",
    "    print(tables_in_pred)\n",
    "    assert len(tables_in_pred) <= 2\n",
    "\n",
    "    token_list = sqlparse.sql.TokenList(wheres)\n",
    "\n",
    "    while True:\n",
    "        index, token = token_list.token_next(index)\n",
    "        if token is None:\n",
    "            break\n",
    "        # print(\"token.value: \", token.value)\n",
    "        if token.value.upper() == \"AND\":\n",
    "            break\n",
    "\n",
    "        match += \" \" + token.value\n",
    "\n",
    "        if (token.value.upper() == \"BETWEEN\"):\n",
    "            # ugh ugliness\n",
    "            index, a = token_list.token_next(index)\n",
    "            index, AND = token_list.token_next(index)\n",
    "            index, b = token_list.token_next(index)\n",
    "            match += \" \" + a.value\n",
    "            match += \" \" + AND.value\n",
    "            match += \" \" + b.value\n",
    "            # Note: important not to break here! Will break when we hit the\n",
    "            # \"AND\" in the next iteration.\n",
    "    print(match)\n",
    "    # print(\"tables: \", tables)\n",
    "    # print(\"match: \", match)\n",
    "    # print(\"tables in pred: \", tables_in_pred)\n",
    "    for table in tables_in_pred:\n",
    "        if table not in tables:\n",
    "            # print(tables)\n",
    "            # print(table)\n",
    "            # pdb.set_trace()\n",
    "            # print(\"returning index, None\")\n",
    "            return index, None\n",
    "\n",
    "    if len(tables_in_pred) == 0:\n",
    "        return index, None\n",
    "\n",
    "    return index, match\n",
    "\n",
    "\n",
    "def find_all_tables_till_keyword(token):\n",
    "    tables = []\n",
    "    # print(\"fattk: \", token)\n",
    "    index = 0\n",
    "    while (True):\n",
    "        if (type(token) == sqlparse.sql.Comparison):\n",
    "            left = token.left\n",
    "            right = token.right\n",
    "            if (type(left) == sqlparse.sql.Identifier):\n",
    "                tables.append(left.get_parent_name())\n",
    "            if (type(right) == sqlparse.sql.Identifier):\n",
    "                tables.append(right.get_parent_name())\n",
    "            break\n",
    "        elif (type(token) == sqlparse.sql.Identifier):\n",
    "            tables.append(token.get_parent_name())\n",
    "            break\n",
    "        try:\n",
    "            index, token = token.token_next(index)\n",
    "            if (\"Literal\" in str(token.ttype)) or token.is_keyword:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def find_filter_clauses(table, wheres):\n",
    "    matched = []\n",
    "    # print(tables)\n",
    "    index = 0\n",
    "    while True:\n",
    "        index, match = find_next_match(table, wheres, index)\n",
    "        # print(\"got index, match: \", index)\n",
    "        # print(match)\n",
    "        if match is not None:\n",
    "            matched.append(match)\n",
    "        if index is None:\n",
    "            break\n",
    "\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e0138c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cn']\n",
      " cn.country_code ='[us]'\n",
      "['k']\n",
      " k.keyword ='character-name-in-title'\n",
      "['n']\n",
      " n.name LIKE 'B%'\n",
      "['n', 'ci']\n",
      " n.id = ci.person_id\n",
      "['ci', 't']\n",
      " ci.movie_id = t.id\n",
      "['t', 'mk']\n",
      " t.id = mk.movie_id\n",
      "['mk', 'k']\n",
      " mk.keyword_id = k.id\n",
      "['t', 'mc']\n",
      " t.id = mc.movie_id\n",
      "['mc', 'cn']\n",
      " mc.company_id = cn.id\n",
      "['ci', 'mc']\n",
      " ci.movie_id = mc.movie_id\n",
      "['ci', 'mk']\n",
      " ci.movie_id = mk.movie_id\n",
      "['mc', 'mk']\n",
      " mc.movie_id = mk.movie_id ;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\" cn.country_code ='[us]'\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_filter_clauses(['cn'], where_clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ef015",
   "metadata": {},
   "outputs": [],
   "source": [
    "be = Bound_ensemble(table_buckets, schema, 1, ground_truth_factors_no_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = model_folder + f\"model_imdb_default.pkl\"\n",
    "pickle.dump(be, open(model_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d167d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziniuw/Desktop/research/Learned_QO/FactorJoin/factorjoin-binned-cards/bins.pkl\", \"rb\") as f:\n",
    "    prev_bins = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49cd09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziniuw/Desktop/research/Learned_QO/data/saved_models/imdb_buckets.pkl\", \"rb\") as f:\n",
    "    curr_bins = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd71ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['kind_type.id', 'info_type.id', 'title.id', 'name.id', 'char_name.id', 'role_type.id', 'comp_cast_type.id', 'keyword.id', 'company_name.id', 'company_type.id'])\n",
      "kind_type.id -----------------------------------------------\n",
      "8 1\n",
      "8 1\n",
      "info_type.id -----------------------------------------------\n",
      "72 3\n",
      "72 3\n",
      "title.id -----------------------------------------------\n",
      "74 1238985\n",
      "74 1224796\n",
      "name.id -----------------------------------------------\n",
      "29 781509\n",
      "27 806247\n",
      "char_name.id -----------------------------------------------\n",
      "28 269322\n",
      "28 269322\n",
      "role_type.id -----------------------------------------------\n",
      "12 1\n",
      "12 1\n",
      "comp_cast_type.id -----------------------------------------------\n",
      "2 2\n",
      "2 1\n",
      "keyword.id -----------------------------------------------\n",
      "86 31359\n",
      "86 31359\n",
      "company_name.id -----------------------------------------------\n",
      "75 80873\n",
      "75 80873\n",
      "company_type.id -----------------------------------------------\n",
      "3 1\n",
      "3 1\n"
     ]
    }
   ],
   "source": [
    "attr = \"title.id\"\n",
    "print(prev_bins.keys())\n",
    "for attr in list(prev_bins.keys()):\n",
    "    print(attr, \"-----------------------------------------------\")\n",
    "    print(len(prev_bins[attr]), len(prev_bins[attr][0]))\n",
    "    print(len(curr_bins[attr].bins), len(curr_bins[attr].bins[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52127703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from Schemas.stats.schema import gen_stats_light_schema\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.binning import identify_key_values, sub_optimal_bucketize, greedy_bucketize, \\\n",
    "                                fixed_start_key_bucketize, get_start_key, naive_bucketize, \\\n",
    "                                Table_bucket, update_bins, apply_binning_to_data_value_count\n",
    "from Join_scheme.bound import Factor\n",
    "\n",
    "\n",
    "data_path = \"/Users/ziniuw/Desktop/research/Learned_QO/data/imdb/{}.csv\"\n",
    "model_folder = \"/Users/ziniuw/Desktop/research/Learned_QO/data/saved_models\"\n",
    "bucket_method = \"fixed_start_key\"\n",
    "sample_size=1000000\n",
    "save_bucket_bins=False\n",
    "seed=0\n",
    "\n",
    "schema = gen_imdb_schema(data_path)\n",
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "data = dict()\n",
    "table_lens = dict()\n",
    "table_key_lens = dict()\n",
    "na_values = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                          quotechar='\"',\n",
    "                          sep=\",\")\n",
    "\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    table_lens[table_obj.table_name] = len(df_rows)\n",
    "    if table_obj.table_name not in na_values:\n",
    "        na_values[table_obj.table_name] = dict()\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            table_key_lens[attr] = table_lens[table_obj.table_name]\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            na_values[table_obj.table_name][attr] = len(data[attr][data[attr] != -1]) / table_lens[\n",
    "                table_obj.table_name]\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr] >= 0]\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_count_var = dict()\n",
    "for attr in data:\n",
    "    #print(attr)\n",
    "    u, v = np.unique(data[attr], return_counts=True)\n",
    "    #print(len(u), len(v), np.var(v))\n",
    "    value_count_var[attr] = np.var(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae42746",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for PK in equivalent_keys:\n",
    "    start_key = get_start_key(equivalent_keys[PK], value_count_var, primary_keys)\n",
    "    print(start_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_bins[\"role_type.id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_bins[\"role_type.id\"].bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prev_bins['title.id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(curr_bins['title.id'].bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c84f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"/Users/ziniuw/Desktop/research/Learned_QO/data/saved_models\"\n",
    "model_path = model_folder + f\"model_imdb_default.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    be = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "be.SPERCENTAGE = 1.0\n",
    "be.query_sample_location = \"../Sampling/materialized_IMDB_JOB_sample/{}/job/all_job/\"\n",
    "query_folder = \"../IMDB-JOB/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries, all_sub_plan_queries_str = get_job_sub_plan_queires(query_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_name = \"19b\"\n",
    "temp = be.get_cardinality_bound_all(all_queries[q_name], all_sub_plan_queries_str[q_name], q_name + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "schema = gen_stats_light_schema(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "sample_rate = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    \n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            print(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr]>=0]\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f876c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data:\n",
    "    sample_rate[k] = 1.0\n",
    "    print(k, len(data[k]), np.sum(np.isnan(data[k])), len(data[k][data[k]<0]))\n",
    "print(primary_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict()\n",
    "for PK in equivalent_keys:\n",
    "    print(equivalent_keys[PK])\n",
    "    group_data = {}\n",
    "    group_sample_rate = {}\n",
    "    for K in equivalent_keys[PK]:\n",
    "        group_data[K] = data[K]\n",
    "        group_sample_rate[K] = sample_rate[K]\n",
    "    d, bucket = naive_bucketize(group_data, sample_rate, n_bins=100, primary_keys=primary_keys, return_data=True)\n",
    "    temp[PK] = bucket\n",
    "    for key in d:\n",
    "        print(np.min(d[key]), np.max(d[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bucket.buckets[bucket.start_key].bins))\n",
    "for k in d:\n",
    "    print(\"     ==      \")\n",
    "    print(k, len(data[k]), np.sum(np.isnan(data[k])), len(data[k][data[k]<0]))\n",
    "    print(len(d[k]), len(np.unique(d[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = temp[\"posts.Id\"]\n",
    "for k in bucket.buckets:\n",
    "    print(\"==============================================================\")\n",
    "    print(k, len(bucket.buckets[k].bin_modes))\n",
    "    print(bucket.buckets[k].bin_modes)\n",
    "    print([len(b) for b in bucket.buckets[k].bins])\n",
    "    print(bucket.buckets[k].bin_means)\n",
    "    print(bucket.buckets[k].bin_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32563005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/Users/ziniuw/Desktop/research/Learned_QO/CE_scheme/\")\n",
    "\n",
    "from Schemas.stats.schema import gen_stats_light_schema\n",
    "from Join_scheme.binning import identify_key_values, sub_optimal_bucketize, greedy_bucketize, Table_bucket\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def timestamp_transorform(time_string, start_date=\"2010-07-19 00:00:00\"):\n",
    "    start_date_int = time.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    time_array = time.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return int(time.mktime(time_array)) - int(time.mktime(start_date_int))\n",
    "\n",
    "\n",
    "def read_table_hdf(table_obj):\n",
    "    \"\"\"\n",
    "    Reads hdf from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    df_rows = pd.read_hdf(table_obj.csv_file_location)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def convert_time_to_int(data_folder):\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_file_location = data_folder + file\n",
    "            df_rows = pd.read_csv(csv_file_location)\n",
    "            for attribute in df_rows.columns:\n",
    "                if \"Date\" in attribute:\n",
    "                    if df_rows[attribute].values.dtype == 'object':\n",
    "                        new_value = []\n",
    "                        for value in df_rows[attribute].values:\n",
    "                            new_value.append(timestamp_transorform(value))\n",
    "                        df_rows[attribute] = new_value\n",
    "            df_rows.to_csv(csv_file_location, index=False)\n",
    "\n",
    "\n",
    "def read_table_csv(table_obj, csv_seperator=',', stats=True):\n",
    "    \"\"\"\n",
    "    Reads csv from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    if stats:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    else:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                              quotechar='\"',\n",
    "                              sep=csv_seperator)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def generate_table_buckets(data, key_attrs, bin_sizes, bin_modes, optimal_buckets):\n",
    "    table_buckets = dict()\n",
    "    for table in data:\n",
    "        table_data = data[table]\n",
    "        table_bucket = Table_bucket(table, key_attrs[table], bin_sizes[table])\n",
    "        for key in key_attrs[table]:\n",
    "            if key in bin_modes and len(bin_modes[key]) != 0:\n",
    "                table_bucket.oned_bin_modes[key] = bin_modes[key]\n",
    "            else:\n",
    "                # this is a primary key\n",
    "                table_bucket.oned_bin_modes[key] = np.ones(table_bucket.bin_sizes[key])\n",
    "        # getting mode for 2D bins\n",
    "        if len(key_attrs[table]) == 2:\n",
    "            key1 = key_attrs[table][0]\n",
    "            key2 = key_attrs[table][1]\n",
    "            res1 = np.zeros((table_bucket.bin_sizes[key1], table_bucket.bin_sizes[key2]))\n",
    "            res2 = np.zeros((table_bucket.bin_sizes[key1], table_bucket.bin_sizes[key2]))\n",
    "            key_data = np.stack((table_data[key1].values, table_data[key2].values), axis=1)\n",
    "            assert table_bucket.bin_sizes[key1] == len(optimal_buckets[key1].bins)\n",
    "            assert table_bucket.bin_sizes[key2] == len(optimal_buckets[key2].bins)\n",
    "            for v1, b1 in enumerate(optimal_buckets[key1].bins):\n",
    "                temp_data = key_data[np.isin(key_data[:, 0], b1)]\n",
    "                if len(temp_data) == 0:\n",
    "                    continue\n",
    "                #print(key1, v1, np.max(np.unique(temp_data[:, 0], return_counts=True)[-1]), table_bucket.oned_bin_modes[key1][v1])\n",
    "                #assert np.max(np.unique(temp_data[:, 0], return_counts=True)[-1]) == table_bucket.oned_bin_modes[key1][\n",
    "                    #v1], f\"{key1} data error at {v1}, with \" \\\n",
    "                     #    f\"{np.max(np.unique(temp_data[:, 0], return_counts=True)[-1])} and \" \\\n",
    "                      #   f\"{table_bucket.oned_bin_modes[key1][v1]}.\"\n",
    "                for v2, b2 in enumerate(optimal_buckets[key2].bins):\n",
    "                    temp_data2 = copy.deepcopy(temp_data[np.isin(temp_data[:, 1], b2)])\n",
    "                    if len(temp_data2) == 0:\n",
    "                        continue\n",
    "                    res1[v1, v2] = np.max(np.unique(temp_data2[:, 0], return_counts=True)[-1])\n",
    "                    res2[v1, v2] = np.max(np.unique(temp_data2[:, 1], return_counts=True)[-1])\n",
    "            table_bucket.twod_bin_modes[key1] = res1\n",
    "            table_bucket.twod_bin_modes[key2] = res2\n",
    "        table_buckets[table] = table_bucket\n",
    "\n",
    "    return table_buckets\n",
    "\n",
    "\n",
    "def process_stats_data(data_path, model_folder, n_bins=500, bucket_method=\"greedy\", save_bucket_bins=False):\n",
    "    \"\"\"\n",
    "    Preprocessing stats data and generate optimal bucket\n",
    "    :param data_path: path to stats data folder\n",
    "    :param n_bins: number of bins (the actually number of bins returned will be smaller than this)\n",
    "    :param bucket_method: choose between \"sub_optimal\" and \"greedy\". Please refer to binning.py for details.\n",
    "    :param save_bucket_bins: Set to true for dynamic environment, the default is False for static environment\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not data_path.endswith(\".csv\"):\n",
    "        data_path += \"/{}.csv\"\n",
    "    schema = gen_stats_light_schema(data_path)\n",
    "    all_keys, equivalent_keys = identify_key_values(schema)\n",
    "    data = dict()\n",
    "    key_data = dict()  # store the columns of all keys\n",
    "    sample_rate = dict()\n",
    "    primary_keys = []\n",
    "    null_values = dict()\n",
    "    key_attrs = dict()\n",
    "    for table_obj in schema.tables:\n",
    "        table_name = table_obj.table_name\n",
    "        null_values[table_name] = dict()\n",
    "        key_attrs[table_name] = []\n",
    "        df_rows = read_table_csv(table_obj, stats=True)\n",
    "        for attr in df_rows.columns:\n",
    "            if attr in all_keys:\n",
    "                key_data[attr] = df_rows[attr].values\n",
    "                # the nan value of id are set to -1, this is hardcoded.\n",
    "                key_data[attr][np.isnan(key_data[attr])] = -1\n",
    "                key_data[attr][key_data[attr] < 0] = -1\n",
    "                null_values[table_name][attr] = -1\n",
    "                key_data[attr] = copy.deepcopy(key_data[attr])[key_data[attr] >= 0]\n",
    "                # if the all keys have exactly one appearance, we consider them primary keys\n",
    "                # we set a error margin of 0.01 in case of data mis-write.\n",
    "                if len(np.unique(key_data[attr])) >= len(key_data[attr]) * 0.99:\n",
    "                    primary_keys.append(attr)\n",
    "                sample_rate[attr] = 1.0\n",
    "                key_attrs[table_name].append(attr)\n",
    "            else:\n",
    "                temp = df_rows[attr].values\n",
    "                null_values[table_name][attr] = np.nanmin(temp) - 100\n",
    "                temp[np.isnan(temp)] = null_values[table_name][attr]\n",
    "        data[table_name] = df_rows\n",
    "\n",
    "    all_bin_modes = dict()\n",
    "    bin_size = dict()\n",
    "    binned_data = dict()\n",
    "    optimal_buckets = dict()\n",
    "    for PK in equivalent_keys:\n",
    "        print(f\"bucketizing equivalent key group:\", equivalent_keys[PK])\n",
    "        group_data = {}\n",
    "        group_sample_rate = {}\n",
    "        for K in equivalent_keys[PK]:\n",
    "            group_data[K] = key_data[K]\n",
    "            group_sample_rate[K] = sample_rate[K]\n",
    "        if bucket_method == \"greedy\":\n",
    "            temp_data, optimal_bucket = greedy_bucketize(group_data, sample_rate, n_bins, primary_keys, True)\n",
    "        elif bucket_method == \"sub_optimal\":\n",
    "            temp_data, optimal_bucket = sub_optimal_bucketize(group_data, sample_rate, n_bins, primary_keys)\n",
    "        else:\n",
    "            assert False, f\"unrecognized bucketization method: {bucket_method}\"\n",
    "\n",
    "        binned_data.update(temp_data)\n",
    "        for K in equivalent_keys[PK]:\n",
    "            optimal_buckets[K] = optimal_bucket\n",
    "            temp_table_name = K.split(\".\")[0]\n",
    "            if temp_table_name not in bin_size:\n",
    "                bin_size[temp_table_name] = dict()\n",
    "            bin_size[temp_table_name][K] = len(optimal_bucket.bins)\n",
    "            all_bin_modes[K] = np.asarray(optimal_bucket.buckets[K].bin_modes)\n",
    "\n",
    "    table_buckets = generate_table_buckets(data, key_attrs, bin_size, all_bin_modes, optimal_buckets)\n",
    "\n",
    "    for K in binned_data:\n",
    "        temp_table_name = K.split(\".\")[0]\n",
    "        temp = data[temp_table_name][K].values\n",
    "        temp[temp >= 0] = binned_data[K]\n",
    "\n",
    "    if save_bucket_bins:\n",
    "        with open(model_folder + f\"/buckets.pkl\") as f:\n",
    "            pickle.dump(optimal_buckets, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49973d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "model_folder = \"/home/ubuntu/data_CE/saved_models\"\n",
    "data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size = process_stats_data(data_path,\n",
    "                                                                       model_folder, 200, \"sub_optimal\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ed6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = pd.read_csv(data_path.format(\"badges\"))\n",
    "df_rows.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6868f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"votes\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b92583",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.twod_bin_modes[attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"postHistory\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba052cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket.oned_bin_modes[bucket.id_attributes[0]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[0]], axis=1))\n",
    "print(bucket.oned_bin_modes[bucket.id_attributes[1]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d89f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bins = np.histogram(a, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ffbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[np.where((a<30) & (a>15))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66498d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa29ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
