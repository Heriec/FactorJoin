{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import psycopg2\n",
    "sys.path.append(\"../\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv, process_stats_data, process_imdb_data\n",
    "from Join_scheme.bound import Bound_ensemble\n",
    "from Join_scheme.join_graph import parse_query_all_join, get_join_hyper_graph\n",
    "from Evaluation.testing import get_job_sub_plan_queires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40118e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname=imdb user=postgres password=postgres host=127.0.0.1 port=5436\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6595a2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2528312,)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM title;\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c728e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3364: DtypeWarning: Columns (5,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3364: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3364: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3364: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title.kind_id\n",
      "movie_info.info_type_id\n",
      "movie_keyword.movie_id\n",
      "cast_info.person_id\n",
      "cast_info.person_role_id\n",
      "cast_info.role_id\n",
      "complete_cast.status_id\n",
      "movie_keyword.keyword_id\n",
      "movie_companies.company_id\n",
      "movie_companies.company_type_id\n",
      "kind_type.id 8\n",
      "info_type.id 72\n",
      "title.id 74\n",
      "name.id 27\n",
      "char_name.id 28\n",
      "role_type.id 12\n",
      "comp_cast_type.id 2\n",
      "keyword.id 86\n",
      "company_name.id 75\n",
      "company_type.id 3\n"
     ]
    }
   ],
   "source": [
    "n_bins = {\n",
    "    'title.id': 800,\n",
    "    'info_type.id': 100,\n",
    "    'keyword.id': 100,\n",
    "    'company_name.id': 100,\n",
    "    'name.id': 100,\n",
    "    'company_type.id': 100,\n",
    "    'comp_cast_type.id': 50,\n",
    "    'kind_type.id': 50,\n",
    "    'char_name.id': 50,\n",
    "    'role_type.id': 50,\n",
    "    'link_type.id': 50\n",
    "}\n",
    "data_path = \"/home/ubuntu/data_CE/imdb/{}.csv\"\n",
    "model_folder = \"/home/ubuntu/data_CE/CE_scheme_models/\"\n",
    "schema, table_buckets, ground_truth_factors_no_filter = process_imdb_data(data_path, model_folder, n_bins,\n",
    "                                                                        \"fixed_start_key\", save_bucket_bins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1fbc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "be = Bound_ensemble(table_buckets, schema, 1, ground_truth_factors_no_filter)\n",
    "model_path = model_folder + f\"model_imdb_default.pkl\"\n",
    "pickle.dump(be, open(model_path, 'wb'), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.binning import identify_key_values, sub_optimal_bucketize, Table_bucket\n",
    "from Join_scheme.binning import apply_binning_to_data_value_count\n",
    "from Join_scheme.bound import Factor\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def timestamp_transorform(time_string, start_date=\"2010-07-19 00:00:00\"):\n",
    "    start_date_int = time.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    time_array = time.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return int(time.mktime(time_array)) - int(time.mktime(start_date_int))\n",
    "\n",
    "\n",
    "def read_table_hdf(table_obj):\n",
    "    \"\"\"\n",
    "    Reads hdf from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    df_rows = pd.read_hdf(table_obj.csv_file_location)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def read_table_csv(table_obj, csv_seperator=',', stats=True):\n",
    "    \"\"\"\n",
    "    Reads csv from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    if stats:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    else:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                              quotechar='\"',\n",
    "                              sep=csv_seperator)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def make_sample(np_data, nrows=1000000, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    samp_data = np_data[np_data != -1]\n",
    "    if len(samp_data) <= nrows:\n",
    "        return samp_data, 1.0\n",
    "    else:\n",
    "        selected = np.random.choice(len(samp_data), size=nrows, replace=False)\n",
    "        return samp_data[selected], nrows/len(samp_data)\n",
    "\n",
    "\n",
    "def stats_analysis(sample, data, sample_rate, show=10):\n",
    "    n, c = np.unique(sample, return_counts=True)\n",
    "    idx = np.argsort(c)[::-1]\n",
    "    for i in range(min(show, len(idx))):\n",
    "        print(c[idx[i]], c[idx[i]]/sample_rate, len(data[data == n[idx[i]]]))\n",
    "\n",
    "\n",
    "def get_ground_truth_no_filter(equivalent_keys, data, bins, table_lens, na_values):\n",
    "    all_factor_pdfs = dict()\n",
    "    for PK in equivalent_keys:\n",
    "        bin_value = bins[PK]\n",
    "        for key in equivalent_keys[PK]:\n",
    "            table = key.split(\".\")[0]\n",
    "            temp = apply_binning_to_data_value_count(bin_value, data[key])\n",
    "            if table not in all_factor_pdfs:\n",
    "                all_factor_pdfs[table] = dict()\n",
    "            all_factor_pdfs[table][key] = temp / np.sum(temp)\n",
    "\n",
    "    all_factors = dict()\n",
    "    for table in all_factor_pdfs:\n",
    "        all_factors[table] = Factor(table, table_lens[table], list(all_factor_pdfs[table].keys()),\n",
    "                                    all_factor_pdfs[table], na_values[table])\n",
    "    return all_factors\n",
    "\n",
    "\n",
    "def process_imdb_data(data_path, model_folder, n_bins, sample_size=100000, save_bucket_bins=False):\n",
    "    schema = gen_imdb_schema(data_path)\n",
    "    all_keys, equivalent_keys = identify_key_values(schema)\n",
    "    data = dict()\n",
    "    table_lens = dict()\n",
    "    na_values = dict()\n",
    "    primary_keys = []\n",
    "    for table_obj in schema.tables:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                              quotechar='\"',\n",
    "                              sep=\",\")\n",
    "\n",
    "        df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "        for attribute in table_obj.irrelevant_attributes:\n",
    "            df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "        df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "        table_lens[table_obj.table_name] = len(df_rows)\n",
    "        if table_obj.table_name not in na_values:\n",
    "            na_values[table_obj.table_name] = dict()\n",
    "        for attr in df_rows.columns:\n",
    "            if attr in all_keys:\n",
    "                data[attr] = df_rows[attr].values\n",
    "                data[attr][np.isnan(data[attr])] = -1\n",
    "                data[attr][data[attr] < 0] = -1\n",
    "                na_values[table_obj.table_name][attr] = len(data[attr][data[attr] != -1]) / table_lens[\n",
    "                    table_obj.table_name]\n",
    "                data[attr] = copy.deepcopy(data[attr])[data[attr] >= 0]\n",
    "                if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                    primary_keys.append(attr)\n",
    "\n",
    "    sample_rate = dict()\n",
    "    sampled_data = dict()\n",
    "    for k in data:\n",
    "        temp = make_sample(data[k], 1000000)\n",
    "        sampled_data[k] = temp[0]\n",
    "        sample_rate[k] = temp[1]\n",
    "\n",
    "    optimal_buckets = dict()\n",
    "    bin_size = dict()\n",
    "    all_bin_modes = dict()\n",
    "    for PK in equivalent_keys:\n",
    "        # if PK != 'kind_type.id':\n",
    "        #   continue\n",
    "        group_data = {}\n",
    "        group_sample_rate = {}\n",
    "        for K in equivalent_keys[PK]:\n",
    "            group_data[K] = sampled_data[K]\n",
    "            group_sample_rate[K] = sample_rate[K]\n",
    "        _, optimal_bucket = sub_optimal_bucketize(group_data, group_sample_rate, n_bins=n_bins[PK], primary_keys=primary_keys)\n",
    "        optimal_buckets[PK] = optimal_bucket\n",
    "        for K in equivalent_keys[PK]:\n",
    "            temp_table_name = K.split(\".\")[0]\n",
    "            if temp_table_name not in bin_size:\n",
    "                bin_size[temp_table_name] = dict()\n",
    "                all_bin_modes[temp_table_name] = dict()\n",
    "            bin_size[temp_table_name][K] = len(optimal_bucket.bins)\n",
    "            all_bin_modes[temp_table_name][K] = optimal_bucket.buckets[K].bin_modes\n",
    "\n",
    "    table_buckets = dict()\n",
    "    for table_name in bin_size:\n",
    "        table_buckets[table_name] = Table_bucket(table_name, list(bin_size[table_name].keys()), bin_size[table_name],\n",
    "                                                 all_bin_modes[table_name])\n",
    "    \n",
    "    all_bins = dict()\n",
    "    for key in optimal_buckets:\n",
    "        all_bins[key] = optimal_buckets[key].bins\n",
    "\n",
    "    ground_truth_factors_no_filter = get_ground_truth_no_filter(equivalent_keys, data, all_bins, table_lens, na_values)\n",
    "\n",
    "    if save_bucket_bins:\n",
    "        with open(model_folder + f\"/imdb_buckets.pkl\", \"wb\") as f:\n",
    "            pickle.dump(optimal_buckets, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return schema, table_buckets, ground_truth_factors_no_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = {\n",
    "    'title.id': 800,\n",
    "    'info_type.id': 100,\n",
    "    'keyword.id': 100,\n",
    "    'company_name.id': 100,\n",
    "    'name.id': 100,\n",
    "    'company_type.id': 100,\n",
    "    'comp_cast_type.id': 50,\n",
    "    'kind_type.id': 50,\n",
    "    'char_name.id': 50,\n",
    "    'role_type.id': 50,\n",
    "    'link_type.id': 50\n",
    "}\n",
    "data_path = \"/home/ubuntu/data_CE/imdb/{}.csv\"\n",
    "model_folder = \"/home/ubuntu/data_CE/CE_scheme_models/\"\n",
    "schema, table_buckets, ground_truth_factors_no_filter = process_imdb_data(data_path, model_folder, n_bins,\n",
    "                                                                              save_bucket_bins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa56181",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ubuntu/CE_scheme/factorjoin-binned-cards/equivalent_keys.pkl\", \"rb\") as f:\n",
    "    equivalent_keys = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76806da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind_type.id': {'aka_title.kind_id', 'kind_type.id', 'title.kind_id'},\n",
       " 'info_type.id': {'info_type.id',\n",
       "  'movie_info.info_type_id',\n",
       "  'movie_info_idx.info_type_id',\n",
       "  'person_info.info_type_id'},\n",
       " 'title.id': {'aka_title.movie_id',\n",
       "  'cast_info.movie_id',\n",
       "  'complete_cast.movie_id',\n",
       "  'movie_companies.movie_id',\n",
       "  'movie_info.movie_id',\n",
       "  'movie_info_idx.movie_id',\n",
       "  'movie_keyword.movie_id',\n",
       "  'title.id'},\n",
       " 'name.id': {'aka_name.person_id',\n",
       "  'cast_info.person_id',\n",
       "  'name.id',\n",
       "  'person_info.person_id'},\n",
       " 'char_name.id': {'cast_info.person_role_id', 'char_name.id'},\n",
       " 'role_type.id': {'cast_info.role_id', 'role_type.id'},\n",
       " 'comp_cast_type.id': {'comp_cast_type.id',\n",
       "  'complete_cast.status_id',\n",
       "  'complete_cast.subject_id'},\n",
       " 'keyword.id': {'keyword.id', 'movie_keyword.keyword_id'},\n",
       " 'company_name.id': {'company_name.id', 'movie_companies.company_id'},\n",
       " 'company_type.id': {'company_type.id', 'movie_companies.company_type_id'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equivalent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ubuntu/data_CE/CE_scheme_models/imdb_buckets.pkl\", \"rb\") as f:\n",
    "    curr_bins = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ea5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in list(curr_bins.keys()):\n",
    "    print(attr, \"-----------------------------------------------\")\n",
    "    print(len(curr_bins[attr].bins), len(curr_bins[attr].bins[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from Join_scheme.join_graph import get_join_hyper_graph, parse_query_all_join\n",
    "from Join_scheme.data_prepare import identify_key_values\n",
    "#from Sampling.load_sample import load_sample_imdb_one_query\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability on one table.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table, table_len, variables, pdfs, na_values=None):\n",
    "        self.table = table\n",
    "        self.table_len = table_len\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.na_values = na_values  # this is the percentage of data, which is not nan.\n",
    "\n",
    "\n",
    "class Group_Factor:\n",
    "    \"\"\"\n",
    "        This the class defines a multidimensional conditional probability on a group of tables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tables, tables_size, variables, pdfs, bin_modes, equivalent_groups=None, \n",
    "                 table_key_equivalent_group=None, na_values=None, join_cond=None):\n",
    "        self.table = tables\n",
    "        self.tables_size = tables_size\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.bin_modes = bin_modes\n",
    "        self.equivalent_groups = equivalent_groups\n",
    "        self.table_key_equivalent_group = table_key_equivalent_group\n",
    "        self.na_values = na_values\n",
    "        self.join_cond = join_cond\n",
    "\n",
    "\n",
    "class Bound_ensemble:\n",
    "    \"\"\"\n",
    "    This the class where we store all the trained models and perform inference on the bound.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table_buckets, schema, ground_truth_factors_no_filter=None):\n",
    "        self.table_buckets = table_buckets\n",
    "        self.schema = schema\n",
    "        self.all_keys, self.equivalent_keys = identify_key_values(schema)\n",
    "        self.all_join_conds = None\n",
    "        self.ground_truth_factors_no_filter = ground_truth_factors_no_filter\n",
    "        # self.reverse_table_alias = None\n",
    "\n",
    "    def parse_query_simple(self, query):\n",
    "        \"\"\"\n",
    "        If your selection query contains no aggregation and nested sub-queries, you can use this function to parse a\n",
    "        join query. Otherwise, use parse_query function.\n",
    "        \"\"\"\n",
    "        tables_all, join_cond, join_keys = parse_query_all_join(query)\n",
    "        # TODO: implement functions on parsing filter conditions.\n",
    "        table_filters = dict()\n",
    "        return tables_all, table_filters, join_cond, join_keys\n",
    "\n",
    "    def get_all_id_conidtional_distribution(self, query_file_name, tables_alias, join_keys):\n",
    "        # TODO: make it work on query-driven and sampling based\n",
    "        return load_sample_imdb_one_query(self.table_buckets, tables_alias, query_file_name, join_keys,\n",
    "                                          self.ground_truth_factors_no_filter)\n",
    "\n",
    "    def eliminate_one_key_group(self, tables, key_group, factors, relevant_keys):\n",
    "        \"\"\"This version only supports 2D distributions (i.e. the distribution learned with tree-structured PGM)\"\"\"\n",
    "        rest_group = None\n",
    "        rest_group_cardinalty = 0\n",
    "        eliminated_tables = []\n",
    "        rest_group_tables = []\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            if len(temp) == 0:\n",
    "                eliminated_tables.append(table)\n",
    "            for key in temp:\n",
    "                if rest_group:\n",
    "                    assert factors[table].cardinalities[key] == rest_group_cardinalty\n",
    "                    rest_group_tables.append(table)\n",
    "                else:\n",
    "                    rest_group = key\n",
    "                    rest_group_cardinalty = factors[table].cardinalities[key]\n",
    "                    rest_group_tables = [table]\n",
    "\n",
    "        all_probs_eliminated = []\n",
    "        all_modes_eliminated = []\n",
    "        for table in eliminated_tables:\n",
    "            bin_modes = self.table_buckets[table].oned_bin_modes[relevant_keys[key_group][table]]\n",
    "            all_probs_eliminated.append(factors[table].pdfs)\n",
    "            all_modes_eliminated.append(np.minimum(bin_modes, factors[table].pdfs))\n",
    "\n",
    "        if rest_group:\n",
    "            new_factor_pdf = np.zeros(rest_group_cardinalty)\n",
    "        else:\n",
    "            return self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for i in range(rest_group_cardinalty):\n",
    "            for table in rest_group_tables:\n",
    "                idx_f = factors[table].equivalent_variables.index(key_group)\n",
    "                idx_b = self.table_buckets[table].id_attributes.index(relevant_keys[key_group][table])\n",
    "                bin_modes = self.table_buckets[table].twod_bin_modes[relevant_keys[key_group][table]]\n",
    "                if idx_f == 0 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 0 and idx_b == 1:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 1 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[i, :]))\n",
    "                else:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[i, :]))\n",
    "            new_factor_pdf[i] = self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for table in rest_group_tables:\n",
    "            factors[table] = Factor([rest_group], new_factor_pdf, [rest_group])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def compute_bound_oned(self, all_probs, all_modes, return_factor=False):\n",
    "        temp_all_modes = []\n",
    "        for i in range(len(all_modes)):\n",
    "            temp_all_modes.append(np.minimum(all_probs[i], all_modes[i]))\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        temp_all_modes = np.stack(temp_all_modes, axis=0)\n",
    "        multiplier = np.prod(temp_all_modes, axis=0)\n",
    "        non_zero_idx = np.where(multiplier != 0)[0]\n",
    "        min_number = np.amin(all_probs[:, non_zero_idx] / temp_all_modes[:, non_zero_idx], axis=0)\n",
    "        # print(min_number, multiplier[non_zero_idx])\n",
    "        if return_factor:\n",
    "            new_probs = np.zeros(multiplier.shape)\n",
    "            new_probs[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return new_probs, multiplier\n",
    "        else:\n",
    "            multiplier[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return np.sum(multiplier)\n",
    "\n",
    "    def get_optimal_elimination_order(self, equivalent_group, join_keys, factors):\n",
    "        \"\"\"\n",
    "        This function determines the optimial elimination order for each key group\n",
    "        \"\"\"\n",
    "        cardinalities = dict()\n",
    "        lengths = dict()\n",
    "        tables_involved = dict()\n",
    "        relevant_keys = dict()\n",
    "        for group in equivalent_group:\n",
    "            relevant_keys[group] = dict()\n",
    "            lengths[group] = len(equivalent_group[group])\n",
    "            cardinalities[group] = []\n",
    "            tables_involved[group] = set([])\n",
    "            for keys in equivalent_group[group]:\n",
    "                for table in join_keys:\n",
    "                    if keys in join_keys[table]:\n",
    "                        cardinalities[group].append(len(join_keys[table]))\n",
    "                        tables_involved[group].add(table)\n",
    "                        variables = factors[table].variables\n",
    "                        variables[variables.index(keys)] = group\n",
    "                        factors[table].variables = variables\n",
    "                        relevant_keys[group][table] = keys\n",
    "                        break\n",
    "            cardinalities[group] = np.asarray(cardinalities[group])\n",
    "\n",
    "        optimal_order = list(equivalent_group.keys())\n",
    "        for i in range(len(optimal_order)):\n",
    "            min_idx = i\n",
    "            for j in range(i + 1, len(optimal_order)):\n",
    "                min_group = optimal_order[min_idx]\n",
    "                curr_group = optimal_order[j]\n",
    "                if np.max(cardinalities[curr_group]) < np.max(cardinalities[min_group]):\n",
    "                    min_idx = j\n",
    "                else:\n",
    "                    min_max_tables = np.max(cardinalities[min_group])\n",
    "                    min_num_max_tables = len(np.where(cardinalities[min_group] == min_max_tables)[0])\n",
    "                    curr_max_tables = np.max(cardinalities[curr_group])\n",
    "                    curr_num_max_tables = len(np.where(cardinalities[curr_group] == curr_max_tables)[0])\n",
    "                    if curr_num_max_tables < min_num_max_tables:\n",
    "                        min_idx = j\n",
    "                    elif lengths[curr_group] < lengths[min_group]:\n",
    "                        min_idx = j\n",
    "            optimal_order[i], optimal_order[min_idx] = optimal_order[min_idx], optimal_order[i]\n",
    "        return optimal_order, tables_involved, relevant_keys\n",
    "\n",
    "    def get_cardinality_bound_one(self, query_str):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group = get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(table_queries, join_keys, equivalent_group)\n",
    "        optimal_order, tables_involved, relevant_keys = self.get_optimal_elimination_order(equivalent_group, join_keys,\n",
    "                                                                                           conditional_factors)\n",
    "\n",
    "        for key_group in optimal_order:\n",
    "            tables = tables_involved[key_group]\n",
    "            res = self.eliminate_one_key_group(tables, key_group, conditional_factors, relevant_keys)\n",
    "        return res\n",
    "\n",
    "    def get_sub_plan_queries_sql(self, query_str, sub_plan_query_str_all, query_name=None):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group = get_join_hyper_graph(join_keys,\n",
    "                                                                                                    self.equivalent_keys)\n",
    "        cached_sub_queries_sql = dict()\n",
    "        cached_union_key_group = dict()\n",
    "        res_sql = []\n",
    "        for (left_tables, right_tables) in sub_plan_query_str_all:\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  # get the string name of the sub plan query\n",
    "            sql_header = \"SELECT COUNT(*) FROM \"\n",
    "            for alias in sub_plan_query_list:\n",
    "                sql_header += (tables_all[alias] + \" AS \" + alias + \", \")\n",
    "            sql_header = sql_header[:-2] + \" WHERE \"\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries_sql, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_sql = cached_sub_queries_sql[right_tables]\n",
    "                right_union_key_group = cached_union_key_group[right_tables]\n",
    "                if left_tables in table_queries:\n",
    "                    left_sql = table_queries[left_tables]\n",
    "                    curr_sql = right_sql + \" AND (\" + left_sql + \")\"\n",
    "                else:\n",
    "                    curr_sql = right_sql\n",
    "                additional_joins, union_key_group = self.get_additional_join_with_table_group(left_tables,\n",
    "                                                                                              right_union_key_group,\n",
    "                                                                                              table_equivalent_group,\n",
    "                                                                                              table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    curr_sql = curr_sql + \" AND \" + join\n",
    "            else:\n",
    "                curr_sql = \"\"\n",
    "                if left_tables in table_queries:\n",
    "                    curr_sql += (\"(\" + table_queries[left_tables] + \")\")\n",
    "                if right_tables in table_queries:\n",
    "                    if curr_sql != \"\":\n",
    "                        curr_sql += \" AND \"\n",
    "                    curr_sql += (\"(\" + table_queries[right_tables] + \")\")\n",
    "\n",
    "                additional_joins, union_key_group = self.get_additional_joins_two_tables(left_tables, right_tables,\n",
    "                                                                                         table_equivalent_group,\n",
    "                                                                                         table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    if curr_sql == \"\":\n",
    "                        curr_sql += join\n",
    "                    else:\n",
    "                        curr_sql = curr_sql + \" AND \" + join\n",
    "            cached_sub_queries_sql[sub_plan_query_str] = curr_sql\n",
    "            cached_union_key_group[sub_plan_query_str] = union_key_group\n",
    "            res_sql.append(sql_header + curr_sql + \";\")\n",
    "        return res_sql\n",
    "\n",
    "    def get_cardinality_bound_all(self, query_str, sub_plan_query_str_all, query_name=None, debug=False,\n",
    "                                  true_card=None):\n",
    "        \"\"\"\n",
    "        Get the cardinality bounds for all sub_plan_queires of a query.\n",
    "        Note: Due to efficiency, this current version only support left_deep plans (like the one generated by postgres),\n",
    "              but it can easily support right deep or bushy plans.\n",
    "        :param query_str: the target query\n",
    "        :param sub_plan_query_str_all: all sub_plan_queries of the target query,\n",
    "               it should be sorted by number of the tables in the sub_plan_query\n",
    "        \"\"\"\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        #print(join_cond)\n",
    "        # print(join_keys)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group, table_key_group_map = \\\n",
    "            get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(query_name, tables_all, join_keys)\n",
    "        # self.reverse_table_alias = {v: k for k, v in tables_all.items()}\n",
    "        cached_sub_queries = dict()\n",
    "        cardinality_bounds = []\n",
    "        for i, (left_tables, right_tables) in enumerate(sub_plan_query_str_all):\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  # get the string name of the sub plan query\n",
    "            # print(sub_plan_query_str)\n",
    "            #print(sub_plan_query_str, \"=========================================\")\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_bound_factor = cached_sub_queries[right_tables]\n",
    "                curr_bound_factor, res = self.join_with_one_table(sub_plan_query_str,\n",
    "                                                                  left_tables,\n",
    "                                                                  tables_all,\n",
    "                                                                  right_bound_factor,\n",
    "                                                                  conditional_factors[left_tables],\n",
    "                                                                  table_equivalent_group,\n",
    "                                                                  table_key_equivalent_group,\n",
    "                                                                  table_key_group_map,\n",
    "                                                                  join_cond)\n",
    "            else:\n",
    "                curr_bound_factor, res = self.join_two_tables(sub_plan_query_str,\n",
    "                                                              left_tables,\n",
    "                                                              right_tables,\n",
    "                                                              tables_all,\n",
    "                                                              conditional_factors,\n",
    "                                                              join_keys,\n",
    "                                                              table_equivalent_group,\n",
    "                                                              table_key_equivalent_group,\n",
    "                                                              table_key_group_map,\n",
    "                                                              join_cond)\n",
    "            cached_sub_queries[sub_plan_query_str] = curr_bound_factor\n",
    "            res = max(res, 1)\n",
    "            if debug:\n",
    "                if true_card[i] == -1:\n",
    "                    error = \"NA\"\n",
    "                else:\n",
    "                    error = max(res / true_card[i], true_card[i] / res)\n",
    "                #print(f\"{left_tables}, {right_tables}|| estimate: {res}, true: {true_card[i]}, error: {error}\")\n",
    "            cardinality_bounds.append(res)\n",
    "        return cardinality_bounds\n",
    "\n",
    "    def join_with_one_table(self, sub_plan_query_str, left_table, tables_all, right_bound_factor, cond_factor_left,\n",
    "                            table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "        Get the cardinality bound by joining the left_table with the seen right_tables\n",
    "        :param left_table:\n",
    "        :param right_tables:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_with_table_group(left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                                table_key_equivalent_group, table_key_group_map, join_cond)\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = right_bound_factor.bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        new_na_values = dict()\n",
    "        right_variables = right_bound_factor.variables\n",
    "        new_variables = copy.deepcopy(right_variables)\n",
    "        res = right_bound_factor.tables_size\n",
    "        #print(\"\\n\")\n",
    "        #print(union_key_group_set)\n",
    "        #print(union_key_group)\n",
    "        for key_group in equivalent_key_group:\n",
    "            #print(key_group, equivalent_key_group[key_group], res)\n",
    "            # print(cond_factor_left.na_values)\n",
    "            # print(right_bound_factor.na_values)\n",
    "            #print(cond_factor_left.pdfs.keys())\n",
    "            #print(right_bound_factor.pdfs.keys())\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][\"left\"]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][\"left\"]]\n",
    "            for key in equivalent_key_group[key_group][\"left\"]:\n",
    "                new_variables[key] = key_group\n",
    "            for key in equivalent_key_group[key_group][\"right\"]:\n",
    "                if key in right_bound_factor.pdfs:\n",
    "                    new_variables[key] = key_group\n",
    "                    all_pdfs.append(right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key])\n",
    "                    all_bin_modes.append(bin_mode_right[key])\n",
    "                else:\n",
    "                    key = right_variables[key]\n",
    "                    all_pdfs.append(right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key])\n",
    "                    all_bin_modes.append(bin_mode_right[key])\n",
    "\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            if res == 0:\n",
    "                res = 10.0\n",
    "                new_pdf[-1] = 1\n",
    "                key_group_pdf[key_group] = new_pdf\n",
    "            else:\n",
    "                key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1\n",
    "        \n",
    "        for group in union_key_group:\n",
    "            if group not in new_union_key_group:\n",
    "                new_union_key_group[group] = []\n",
    "            for table, keys in union_key_group[group]:\n",
    "                for key in keys:\n",
    "                    new_union_key_group[group].append(key)\n",
    "                    if table == \"left\":\n",
    "                        key_group_pdf[key] = cond_factor_left.pdfs[key]\n",
    "                        key_group_bin_mode[key] = self.table_buckets[tables_all[left_table]].oned_bin_modes[key]\n",
    "                        new_na_values[key] = cond_factor_left.na_values[key]\n",
    "                    else:\n",
    "                        key_group_pdf[key] = right_bound_factor.pdfs[key]\n",
    "                        key_group_bin_mode[key] = right_bound_factor.bin_modes[key]\n",
    "                        new_na_values[key] = right_bound_factor.na_values[key]\n",
    "        \n",
    "        #print(\"****\", key_group_pdf.keys())\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, new_variables, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values, new_join_cond)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_with_table_group(self, left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                       table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in right_bound_factor.join_cond:\n",
    "                actual_join_cond.append(cond)\n",
    "        #print(join_cond[left_table], right_bound_factor.join_cond)\n",
    "        #print(actual_join_cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(right_bound_factor.equivalent_groups)\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = right_bound_factor.join_cond.union(join_cond[left_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = tables_all[left_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key2.split(\".\")[0]\n",
    "                    key_right = tables_all[right_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = tables_all[left_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key1.split(\".\")[0]\n",
    "                    key_right = tables_all[right_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "            \n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    new_left_key = []\n",
    "                    for key in table_key_equivalent_group[left_table][group]:\n",
    "                        if key not in equivalent_key_group[group][\"left\"]:\n",
    "                            new_left_key.append(key)\n",
    "                    if len(new_left_key) != 0:\n",
    "                        union_key_group[group] = [(\"left\", new_left_key)]\n",
    "                    new_right_key = []\n",
    "                    for key in right_bound_factor.table_key_equivalent_group[group]:\n",
    "                        if key not in equivalent_key_group[group][\"right\"]:\n",
    "                            new_right_key.append(key)\n",
    "                    if len(new_right_key) != 0:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"right\", new_right_key))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"right\", new_right_key)]\n",
    "                else:\n",
    "                    if group in table_key_equivalent_group[left_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"left\", table_key_equivalent_group[left_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"left\", table_key_equivalent_group[left_table][group])]\n",
    "                    if group in right_bound_factor.table_key_equivalent_group:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"right\", right_bound_factor.table_key_equivalent_group[group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"right\", right_bound_factor.table_key_equivalent_group[group])]\n",
    "                        \n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(right_bound_factor.equivalent_groups)\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][\"left\"] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][\"right\"] = right_bound_factor.table_key_equivalent_group[group]\n",
    "                else:\n",
    "                    if group in table_key_equivalent_group[left_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"left\", table_key_equivalent_group[left_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"left\", table_key_equivalent_group[left_table][group])]\n",
    "                    if group in right_bound_factor.table_key_equivalent_group:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"right\", right_bound_factor.table_key_equivalent_group[group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"right\", right_bound_factor.table_key_equivalent_group[group])]\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_join_with_table_group(self, left_table, right_union_key_group, table_equivalent_group,\n",
    "                                             table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(set(right_union_key_group.keys()))\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(set(right_union_key_group.keys()))\n",
    "        union_key_group = copy.deepcopy(right_union_key_group)\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = right_union_key_group[group]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group not in union_key_group:\n",
    "                assert group in table_key_equivalent_group[left_table]\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def join_two_tables(self, sub_plan_query_str, left_table, right_table, tables_all, conditional_factors, join_keys,\n",
    "                        table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the cardinality bound by joining the left_table with the right_table\n",
    "            :param left_table:\n",
    "            :param right_table:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_two_tables(left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                          table_key_group_map, join_cond, join_keys, tables_all)\n",
    "        # print(left_table, right_table)\n",
    "        # print(equivalent_key_group)\n",
    "        # print(union_key_group)\n",
    "        # print(conditional_factors.keys())\n",
    "        cond_factor_left = conditional_factors[left_table]\n",
    "        cond_factor_right = conditional_factors[right_table]\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = self.table_buckets[tables_all[right_table]].oned_bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        res = cond_factor_right.table_len\n",
    "        new_na_values = dict()\n",
    "        new_variables = dict()\n",
    "        # print(equivalent_key_group)\n",
    "        for key_group in equivalent_key_group:\n",
    "            # print(key_group)\n",
    "            # print(\"========================\")\n",
    "            #print(equivalent_key_group[key_group][left_table])\n",
    "            #print(bin_mode_left)\n",
    "            # print(\"==========================================\")\n",
    "            #print(equivalent_key_group[key_group][right_table])\n",
    "            #print(bin_mode_right)\n",
    "            if len(equivalent_key_group[key_group][left_table]) > 1:\n",
    "                print(len(equivalent_key_group[key_group][left_table]), sub_plan_query_str)\n",
    "            if len(equivalent_key_group[key_group][right_table]) > 1:\n",
    "                print(len(equivalent_key_group[key_group][right_table]), sub_plan_query_str)\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                       [cond_factor_right.pdfs[key] * res * cond_factor_right.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][right_table]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                            [bin_mode_right[key] for key in equivalent_key_group[key_group][right_table]]\n",
    "            # print(\"====================================================\")\n",
    "            # print(equivalent_key_group[key_group][left_table])\n",
    "            # print(equivalent_key_group[key_group][right_table])\n",
    "            for key in equivalent_key_group[key_group][left_table] + equivalent_key_group[key_group][right_table]:\n",
    "                new_variables[key] = key_group\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1.0\n",
    "\n",
    "        for group in union_key_group:\n",
    "            if group not in new_union_key_group:\n",
    "                new_union_key_group[group] = []\n",
    "            for table, keys in union_key_group[group]:\n",
    "                for key in keys:\n",
    "                    new_union_key_group[group].append(key)\n",
    "                    key_group_pdf[key] = conditional_factors[table].pdfs[key]\n",
    "                    key_group_bin_mode[key] = self.table_buckets[tables_all[table]].oned_bin_modes[key]\n",
    "                    new_na_values[key] = conditional_factors[table].na_values[key]\n",
    "        \n",
    "        #print(\"!!!!!!!\", union_key_group)\n",
    "        #print(new_union_key_group)\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, new_variables, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values, new_join_cond)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_two_tables(self, left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                 table_key_group_map, join_cond, join_keys, tables_all):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in join_cond[right_table]:\n",
    "                actual_join_cond.append(cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = join_cond[left_table].union(join_cond[right_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = tables_all[left_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key2.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = tables_all[right_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = tables_all[left_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key1.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = tables_all[right_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "\n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    new_left_key = []\n",
    "                    for key in table_key_equivalent_group[left_table][group]:\n",
    "                        if key not in equivalent_key_group[group][left_table]:\n",
    "                            new_left_key.append(key)\n",
    "                    if len(new_left_key) != 0:\n",
    "                        union_key_group[group] = [(left_table, new_left_key)]\n",
    "                    new_right_key = []\n",
    "                    for key in table_key_equivalent_group[right_table][group]:\n",
    "                        if key not in equivalent_key_group[group][right_table]:\n",
    "                            new_right_key.append(key)\n",
    "                    if len(new_right_key) != 0:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((right_table, new_right_key))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(right_table, new_right_key)]\n",
    "                else:\n",
    "                    if group in table_key_equivalent_group[left_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((left_table, table_key_equivalent_group[left_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(left_table, table_key_equivalent_group[left_table][group])]\n",
    "                    if group in table_key_equivalent_group[right_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((right_table, table_key_equivalent_group[right_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(right_table, table_key_equivalent_group[right_table][group])]\n",
    "\n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][left_table] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][right_table] = table_key_equivalent_group[right_table][group]\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    if group in union_key_group:\n",
    "                        union_key_group[group].append((left_table, table_key_equivalent_group[left_table][group]))\n",
    "                    else:\n",
    "                        union_key_group[group] = [(left_table, table_key_equivalent_group[left_table][group])]\n",
    "                else:\n",
    "                    if group in union_key_group:\n",
    "                        union_key_group[group].append((right_table, table_key_equivalent_group[right_table][group]))\n",
    "                    else:\n",
    "                        union_key_group[group] = [(right_table, table_key_equivalent_group[right_table][group])]\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_joins_two_tables(self, left_table, right_table, table_equivalent_group,\n",
    "                                        table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group in table_key_equivalent_group[left_table]:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "            else:\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                union_key_group[group] = right_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def get_sub_plan_join_key(self, sub_plan_query, join_cond):\n",
    "        # returning a subset of join_keys covered by the tables in sub_plan_query\n",
    "        touched_join_cond = set()\n",
    "        untouched_join_cond = set()\n",
    "        for tab in join_cond:\n",
    "            if tab in sub_plan_query:\n",
    "                touched_join_cond = touched_join_cond.union(join_cond[tab])\n",
    "            else:\n",
    "                untouched_join_cond = untouched_join_cond.union(join_cond[tab])\n",
    "        touched_join_cond -= untouched_join_cond\n",
    "\n",
    "        join_keys = dict()\n",
    "        for cond in touched_join_cond:\n",
    "            key1 = cond.split(\"=\")[0].strip()\n",
    "            table1 = key1.split(\".\")[0].strip()\n",
    "            if table1 not in join_keys:\n",
    "                join_keys[table1] = set([key1])\n",
    "            else:\n",
    "                join_keys[table1].add(key1)\n",
    "\n",
    "            key2 = cond.split(\"=\")[1].strip()\n",
    "            table2 = key2.split(\".\")[0].strip()\n",
    "            if table2 not in join_keys:\n",
    "                join_keys[table2] = set([key2])\n",
    "            else:\n",
    "                join_keys[table2].add(key2)\n",
    "\n",
    "        return join_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "be = Bound_ensemble(table_buckets, schema, ground_truth_factors_no_filter)\n",
    "model_path = model_folder + f\"model_imdb_default.pkl\"\n",
    "pickle.dump(be, open(model_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"models save at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = \"/home/ubuntu/data_CE/job/\"\n",
    "queries = []\n",
    "q_file_names = []\n",
    "for query_no in range(1, 34):\n",
    "    for suffix in ['a', 'b', 'c', 'd', 'e', 'f', 'g']:\n",
    "        file = f\"{query_no}{suffix}.sql\"\n",
    "        if file in os.listdir(query_path):\n",
    "            q_file_names.append(file.split(\".sql\")[0])\n",
    "            with open(query_path+file, \"r\") as f:\n",
    "                q = f.readline()\n",
    "                queries.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10939d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)\n",
    "print(len(equivalent_keys))\n",
    "print(len(all_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_join_key_appearance(queries, equivalent_keys):\n",
    "    \"\"\"\n",
    "    analyze the workload and count how many times each join key group appears\n",
    "    \"\"\"\n",
    "    all_join_keys_stats = dict()\n",
    "    total_num_appearance = 0\n",
    "    for q in queries:\n",
    "        res = parse_query_all_join(q)\n",
    "        for table in res[-1]:\n",
    "            for join_key in list(res[-1][table]):\n",
    "                for PK in equivalent_keys:\n",
    "                    if join_key in equivalent_keys[PK]:\n",
    "                        total_num_appearance += 1\n",
    "                        if PK in all_join_keys_stats:\n",
    "                            all_join_keys_stats[PK] += 1\n",
    "                        else:\n",
    "                            all_join_keys_stats[PK] = 1\n",
    "                        break\n",
    "    return all_join_keys_stats, total_num_appearance\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_join_keys_stats, total_num_appearance = count_join_key_appearance(queries, equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = {\n",
    "    'title.id': 800,\n",
    "    'info_type.id': 100,\n",
    "    'keyword.id': 100,\n",
    "    'company_name.id': 100,\n",
    "    'name.id': 100,\n",
    "    'company_type.id': 100,\n",
    "    'comp_cast_type.id': 50,\n",
    "    'kind_type.id': 50,\n",
    "    'char_name.id': 50,\n",
    "    'role_type.id': 50,\n",
    "    'link_type.id': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample(np_data, nrows=1000000, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    samp_data = np_data[np_data != -1]\n",
    "    if len(samp_data) <= nrows:\n",
    "        return samp_data, 1.0\n",
    "    else:\n",
    "        selected = np.random.choice(len(samp_data), size=nrows, replace=False)\n",
    "        return samp_data[selected], nrows/len(samp_data)\n",
    "\n",
    "def stats_analysis(sample, data, sample_rate, show=10):\n",
    "    n, c = np.unique(sample, return_counts=True)\n",
    "    idx = np.argsort(c)[::-1]\n",
    "    for i in range(min(show, len(idx))):\n",
    "        print(c[idx[i]], c[idx[i]]/sample_rate, len(data[data == n[idx[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "table_len = dict()\n",
    "na_values = dict()\n",
    "sample_rate = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8', quotechar='\"',\n",
    "                          sep=\",\")\n",
    "    \n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    table_len[table_obj.table_name] = len(df_rows)\n",
    "    if table_obj.table_name not in na_values:\n",
    "        na_values[table_obj.table_name] = dict()\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            print(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            na_values[table_obj.table_name][attr] = len(data[attr][data[attr] != -1])/table_len[table_obj.table_name]\n",
    "            print(len(data[attr]), na_values[table_obj.table_name][attr])\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr]>=0]\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = dict()\n",
    "sampled_data = dict()\n",
    "for k in data:\n",
    "    print(k)\n",
    "    temp = make_sample(data[k], 1000000)\n",
    "    stats_analysis(temp[0], data[k], temp[1])\n",
    "    sampled_data[k] = temp[0]\n",
    "    sample_rate[k] = temp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3302651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class Bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization of a key attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bins=[], bin_modes=[], bin_vars=[], bin_means=[], rest_bins_remaining=None):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        self.bin_modes = bin_modes\n",
    "        self.bin_vars = bin_vars\n",
    "        self.bin_means = bin_means\n",
    "        self.rest_bins_remaining = rest_bins_remaining\n",
    "        if len(bins) != 0:\n",
    "            assert len(bins) == len(bin_modes)\n",
    "\n",
    "\n",
    "class Table_bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization for all key attributes in a table.\n",
    "    Supporting more than three dimensional bin modes requires simplifying the causal structure\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name, id_attributes, bin_sizes, oned_bin_modes=None):\n",
    "        self.table_name = table_name\n",
    "        self.id_attributes = id_attributes\n",
    "        self.bin_sizes = bin_sizes\n",
    "        if oned_bin_modes:\n",
    "            self.oned_bin_modes = oned_bin_modes\n",
    "        else:\n",
    "            self.oned_bin_modes = dict()\n",
    "        self.twod_bin_modes = dict()\n",
    "\n",
    "\n",
    "class Bucket_group:\n",
    "    \"\"\"\n",
    "    The class of bucketization for a group of equivalent join keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buckets, start_key, sample_rate, bins=None, primary_keys=[]):\n",
    "        self.buckets = buckets\n",
    "        self.start_key = start_key\n",
    "        self.sample_rate = sample_rate\n",
    "        self.bins = bins\n",
    "        self.primary_keys = primary_keys\n",
    "\n",
    "    def bucketize(self, data):\n",
    "        \"\"\"\n",
    "        Discretize data based on the bucket\n",
    "        \"\"\"\n",
    "        res = dict()\n",
    "        seen_remain_key = np.array([])\n",
    "        cumulative_bin = copy.deepcopy(self.buckets[self.start_key].bins)\n",
    "        start_means = np.asarray(self.buckets[self.start_key].bin_means)\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            if key != self.start_key:\n",
    "                unique_remain = np.setdiff1d(self.buckets[key].rest_bins_remaining, seen_remain_key)\n",
    "                assert sum([np.sum(np.isin(unique_remain, b) == 1) for b in cumulative_bin]) == 0\n",
    "\n",
    "                if len(unique_remain) != 0:\n",
    "                    remaining_data = data[key][np.isin(data[key], unique_remain)]\n",
    "                    unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "                    unique_counts = np.unique(count_remain)\n",
    "                    for u in unique_counts:\n",
    "                        temp_idx = np.searchsorted(start_means, u)\n",
    "                        if temp_idx == len(cumulative_bin):\n",
    "                            idx = -1\n",
    "                            if u > self.buckets[key].bin_modes[-1]:\n",
    "                                self.buckets[key].bin_modes[-1] = u\n",
    "                        elif temp_idx == 0:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            if (u - start_means[temp_idx - 1]) >= (start_means[temp_idx] - u):\n",
    "                                idx = temp_idx - 1\n",
    "                            else:\n",
    "                                idx = temp_idx\n",
    "                        temp_unique = unique_remain[count_remain == u]\n",
    "                        cumulative_bin[idx] = np.concatenate((cumulative_bin[idx], temp_unique))\n",
    "                        seen_remain_key = np.concatenate((seen_remain_key, temp_unique))\n",
    "                        if u > self.buckets[key].bin_modes[idx]:\n",
    "                            self.buckets[key].bin_modes[idx] = u\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            count = 0\n",
    "            for i, b in enumerate(cumulative_bin):\n",
    "                count += len(data[key][np.isin(data[key], b)])\n",
    "                res[key][np.isin(data[key], b)] = i\n",
    "\n",
    "        self.bins = cumulative_bin\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                res[key] = self.bucketize_PK(data[key])\n",
    "                self.buckets[key] = Bucket(key, bin_modes=np.ones(len(self.bins)))\n",
    "        \n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            if self.sample_rate[key] < 1.0:\n",
    "                bin_modes = np.asarray(self.buckets[key].bin_modes)\n",
    "                bin_modes[bin_modes != 1] = bin_modes[bin_modes != 1] / self.sample_rate[key]\n",
    "                self.buckets[key].bin_modes = bin_modes\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def bucketize_PK(self, data):\n",
    "        res = copy.deepcopy(data)\n",
    "        remaining_data = np.unique(data)\n",
    "        for i, b in enumerate(self.bins):\n",
    "            res[np.isin(data, b)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, b)\n",
    "        if len(remaining_data) != 0:\n",
    "            self.bins.append(list(remaining_data))\n",
    "            for key in self.buckets:\n",
    "                if key not in self.primary_keys:\n",
    "                    self.buckets[key].bin_modes = np.append(self.buckets[key].bin_modes, 0)\n",
    "        res[np.isin(data, remaining_data)] = len(self.bins)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys\n",
    "\n",
    "\n",
    "def equal_freq_binning(name, data, n_bins, data_len):\n",
    "    uniques, counts = data\n",
    "    if len(uniques) <= n_bins:\n",
    "        bins = []\n",
    "        bin_modes = []\n",
    "        bin_vars = []\n",
    "        bin_means = []\n",
    "        \n",
    "        for i, uni in enumerate(uniques):\n",
    "            bins.append([uni])\n",
    "            bin_modes.append(counts[i])\n",
    "            bin_vars.append(0)\n",
    "            bin_means.append(counts[i])\n",
    "        return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "    \n",
    "    unique_counts, count_counts = np.unique(counts, return_counts=True)\n",
    "    idx = np.argsort(unique_counts)\n",
    "    unique_counts = unique_counts[idx]\n",
    "    count_counts = count_counts[idx]\n",
    "\n",
    "    bins = []\n",
    "    bin_modes = []\n",
    "    bin_vars = []\n",
    "    bin_means = []\n",
    "\n",
    "    bin_freq = data_len / n_bins\n",
    "    cur_freq = 0\n",
    "    cur_bin = []\n",
    "    cur_bin_count = []\n",
    "    for i, uni_c in enumerate(unique_counts):\n",
    "        cur_freq += count_counts[i] * uni_c\n",
    "        cur_bin.append(uniques[np.where(counts == uni_c)[0]])\n",
    "        cur_bin_count.extend([uni_c] * count_counts[i])\n",
    "        if (cur_freq > bin_freq) or (i == (len(unique_counts) - 1)):\n",
    "            bins.append(np.concatenate(cur_bin))\n",
    "            cur_bin_count = np.asarray(cur_bin_count)\n",
    "            bin_modes.append(uni_c)\n",
    "            bin_means.append(np.mean(cur_bin_count))\n",
    "            bin_vars.append(np.var(cur_bin_count))\n",
    "            cur_freq = 0\n",
    "            cur_bin = []\n",
    "            cur_bin_count = []\n",
    "    assert len(uniques) == sum([len(b) for b in bins]), f\"some unique values missed or duplicated\"\n",
    "    return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "\n",
    "\n",
    "def compute_variance_score(buckets):\n",
    "    \"\"\"\n",
    "    compute the variance of products of random variables\n",
    "    \"\"\"\n",
    "    all_mean = np.asarray([buckets[k].bin_means for k in buckets])\n",
    "    all_var = np.asarray([buckets[k].bin_vars for k in buckets])\n",
    "    return np.sum(np.prod(all_var + all_mean ** 2, axis=0) - np.prod(all_mean, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def sub_optimal_bucketize(data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    best_variance_score = np.infty\n",
    "    best_bin_len = 0\n",
    "    best_start_key = None\n",
    "    best_buckets = None\n",
    "    for start_key in data:\n",
    "        if start_key in primary_keys:\n",
    "            continue\n",
    "        start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "        rest_buckets = dict()\n",
    "        for key in data:\n",
    "            if key == start_key or key in primary_keys:\n",
    "                continue\n",
    "            uniques = unique_values[key][0]\n",
    "            counts = unique_values[key][1]\n",
    "            rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                       [0] * len(start_bucket.bins), uniques)\n",
    "            for i, bin in enumerate(start_bucket.bins):\n",
    "                idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "                if len(idx) != 0:\n",
    "                    bin_count = counts[idx]\n",
    "                    unique_bin_keys = uniques[idx]\n",
    "                    # unique_bin_count = np.unique(bin_count)\n",
    "                    # bin_count = np.concatenate([counts[counts == j] for j in unique_bin_count])\n",
    "                    # unique_bin_keys = np.concatenate([uniques[counts == j] for j in unique_bin_count])\n",
    "                    rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                         unique_bin_keys)\n",
    "                    rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                    rest_buckets[key].bin_vars[i] = np.var(bin_count)\n",
    "                    rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "        rest_buckets[start_key] = start_bucket\n",
    "        var_score = compute_variance_score(rest_buckets)\n",
    "        if len(start_bucket.bins) > best_bin_len:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "        elif len(start_bucket.bins) >= best_bin_len * 0.9 and var_score < best_variance_score:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "    \n",
    "    best_buckets = Bucket_group(best_buckets, best_start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def fixed_start_key_bucketize(start_key, data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys based on the pre-defined start_key.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "    rest_buckets = dict()\n",
    "    for key in data:\n",
    "        if key == start_key or key in primary_keys:\n",
    "            continue\n",
    "        uniques = unique_values[key][0]\n",
    "        counts = unique_values[key][1]\n",
    "        rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                   [0] * len(start_bucket.bins), uniques)\n",
    "        for i, bin in enumerate(start_bucket.bins):\n",
    "            idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "            if len(idx) != 0:\n",
    "                bin_count = counts[idx]\n",
    "                unique_bin_keys = uniques[idx]\n",
    "                rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                     unique_bin_keys)\n",
    "                rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "    best_buckets = Bucket_group(rest_buckets, start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = gen_imdb_schema(data_path)\n",
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "data = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                          quotechar='\"',\n",
    "                          sep=\",\")\n",
    "\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr] >= 0]\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "\n",
    "sample_rate = dict()\n",
    "sampled_data = dict()\n",
    "for k in data:\n",
    "    temp = make_sample(data[k], 1000000)\n",
    "    sampled_data[k] = temp[0]\n",
    "    sample_rate[k] = temp[1]\n",
    "\n",
    "optimal_buckets = dict()\n",
    "bin_size = dict()\n",
    "all_bin_modes = dict()\n",
    "for PK in equivalent_keys:\n",
    "    # if PK != 'kind_type.id':\n",
    "    #   continue\n",
    "    group_data = {}\n",
    "    group_sample_rate = {}\n",
    "    for K in equivalent_keys[PK]:\n",
    "        group_data[K] = sampled_data[K]\n",
    "        group_sample_rate[K] = sample_rate[K]\n",
    "    _, optimal_bucket = sub_optimal_bucketize(group_data, group_sample_rate, n_bins=n_bins[PK], primary_keys=primary_keys)\n",
    "    optimal_buckets[PK] = optimal_bucket\n",
    "    for K in equivalent_keys[PK]:\n",
    "        temp_table_name = K.split(\".\")[0]\n",
    "        if temp_table_name not in bin_size:\n",
    "            bin_size[temp_table_name] = dict()\n",
    "            all_bin_modes[temp_table_name] = dict()\n",
    "        bin_size[temp_table_name][K] = len(optimal_bucket.bins)\n",
    "        all_bin_modes[temp_table_name][K] = optimal_bucket.buckets[K].bin_modes\n",
    "\n",
    "table_buckets = dict()\n",
    "for table_name in bin_size:\n",
    "    table_buckets[table_name] = Table_bucket(table_name, list(bin_size[table_name].keys()), bin_size[table_name],\n",
    "                                             all_bin_modes[table_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32238d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bins = dict()\n",
    "for key in optimal_buckets:\n",
    "    temp_bins[key] = optimal_buckets[key].bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcdc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bins.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"table_buckets.pkl\", \"rb\") as f:\n",
    "    old_table_buckets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_buckets = dict()\n",
    "for key in table_buckets:\n",
    "    if key in old_table_buckets:\n",
    "        new_table_buckets[key] = old_table_buckets[key]\n",
    "    else:\n",
    "        new_table_buckets[key] = table_buckets[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9131967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_table_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_binning_to_data_value_count(bins, data):\n",
    "    res = np.zeros(len(bins))\n",
    "    bin_mode = np.zeros(len(bins))\n",
    "    unique_remain = np.unique(data)\n",
    "    for i, bin in enumerate(bins):\n",
    "        data_bin = data[np.isin(data, bin)]\n",
    "        res[i] = np.sum(np.isin(data, bin))\n",
    "        unique_remain = np.setdiff1d(unique_remain, bin)\n",
    "        _, counts = np.unique(data_bin, return_counts=True)\n",
    "        if len(counts) == 0:\n",
    "            bin_mode[i] = 0\n",
    "        else:\n",
    "            bin_mode[i] = np.max(counts)\n",
    "    res[0] += np.sum(np.isin(data, unique_remain))\n",
    "    return bin_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = apply_binning_to_data_value_count(bins['title.id'], data['movie_link.linked_movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2479afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ed8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_buckets['movie_link'].oned_bin_modes['movie_link.linked_movie_id'] = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ced593",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_buckets['movie_link'].oned_bin_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab05830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_binning_to_data_value_count(bins, data):\n",
    "    res = np.zeros(len(bins))\n",
    "    unique_remain = np.unique(data)\n",
    "    for i, bin in enumerate(bins):\n",
    "        res[i] = np.sum(np.isin(data, bin))\n",
    "        unique_remain = np.setdiff1d(unique_remain, bin)\n",
    "\n",
    "    res[0] += np.sum(np.isin(data, unique_remain))\n",
    "    return res\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability on one table.\n",
    "    \"\"\"\n",
    "    def __init__(self, table, table_len, variables, pdfs, na_values=None):\n",
    "        self.table = table\n",
    "        self.table_len = table_len\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.na_values = na_values  # this is the percentage of data, which is not nan.\n",
    "\n",
    "all_factor_pdfs = dict()\n",
    "for PK in equivalent_keys:\n",
    "    if PK in bins:\n",
    "        bin_value = bins[PK]\n",
    "    else:\n",
    "        bin_value = temp_bins[PK]\n",
    "    for key in equivalent_keys[PK]:\n",
    "        table = key.split(\".\")[0]\n",
    "        print(table, PK)\n",
    "        temp = apply_binning_to_data_value_count(bin_value, data[key])\n",
    "        print(np.sum(temp))\n",
    "        print(temp)\n",
    "        if table not in all_factor_pdfs:\n",
    "            all_factor_pdfs[table] = dict()\n",
    "        all_factor_pdfs[table][key] = temp / np.sum(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = dict()\n",
    "for table in all_factor_pdfs:\n",
    "    all_factors[table] = Factor(table, table_len[table], list(all_factor_pdfs[table].keys()),\n",
    "                                all_factor_pdfs[table], na_values[table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factor_pdfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899412f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"new_table_buckets.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_table_buckets, f, pickle.HIGHEST_PROTOCOL)\n",
    "#with open(\"ground_truth_factors_no_filter.pkl\", \"wb\") as f:\n",
    " #   pickle.dump(all_factors, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5450365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/home/ubuntu/data_CE/saved_models/bins.pkl\", \"rb\") as f:\n",
    "    bins = pickle.load(f)\n",
    "#with open(\"equivalent_keys.pkl\", \"rb\") as f:\n",
    " #   equivalent_keys = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in temp_bins:\n",
    "    print(key, len(temp_bins[key]), len(bins[key]))\n",
    "    for i in range(len(temp_bins[key])):\n",
    "        #print(key, len(temp_bins[key][i]), len(bins[key][i]))\n",
    "        if np.all(temp_bins[key][i] == bins[key][i]):\n",
    "            print(key, len(temp_bins[key][i]), len(bins[key][i]), \"okay\")\n",
    "        else:\n",
    "            print(key, len(temp_bins[key][i]), len(bins[key][i]), \"notokay\")\n",
    "        #assert np.all(temp_bins[key][i] == bins[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79350c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(temp_bins[k]) for k in temp_bins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = temp['company_type.id']\n",
    "for k in bucket.buckets:\n",
    "    print(\"==============================================================\")\n",
    "    print(k, len(bucket.buckets[k].bin_modes), len(bucket.bins))\n",
    "    print(bucket.buckets[k].bin_modes)\n",
    "    print([len(b) for b in bucket.buckets[k].bins])\n",
    "    print(bucket.buckets[k].bin_means)\n",
    "    print(bucket.buckets[k].bin_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49973d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "model_folder = \"../../CE_scheme_models\"\n",
    "data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size = process_stats_data(data_path,\n",
    "                                                                       model_folder, 200, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6868f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"votes\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"postLinks\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba052cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket.oned_bin_modes[bucket.id_attributes[0]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[0]], axis=1))\n",
    "print(bucket.oned_bin_modes[bucket.id_attributes[1]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d89f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jenkspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = jenkspy.jenks_breaks(np.random.randint(0, 10, 200), nb_class=5)\n",
    "print(breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mode(a).count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f23196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
